---
title: 'Pre-process TSCA2019 MEA Acute data'
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
    df_print: paged
date: "May 11, 2023"
---

# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
rm(list = ls())
library(data.table)
library(openxlsx)
library(stringi)

print(sessionInfo())
```


# User-defined variables

```{r}
start.dir <- "L:/Lab/NHEERL_MEA/Project TSCA 2019/Acute TSCA Conc Response"
dataset_title <- "TSCA2019" # e.g. "name2020"
select.neural.stats.files <- F # select new neural stats files, or use the files in the most recent neural_stats_files_log?
select.calculations.files <- F # select new calculations files, or use the files in the most recent calculations_files_log?
select.raw.cytotox.files <- F # select new raw cytotoxicity files, or use the files in the most recent raw_cytotox_files_log?
run.type.tag.location <- NULL # neural stats files should be named as "tag1_tag2_tag3_....csv". Which tag in the file names defines the run type?
spidmap_file <- "L:/Lab/NHEERL_MEA/Project TSCA 2019/EPA_25092_EPA-Shafer_339_20190722.xlsx"
use_sheet <- 1 # sheet name in spidmap_file

# optional adjutsment; usually can use defaults:
override_wllq_checks <- FALSE # set to TRUE only if you have already verified your wllq updates
plate.id.tag.location <- numeric(0) # only update this if you have to, if your dataset does not include plate.id.tag in file headers
noisy_functions <- FALSE
standard_analysis_duration_requirement <- TRUE # default should be true (recordings that are shorten than this length will be set to wllq == 0)
```


Set up folders, source functions, and load table
```{r}
if (!dir.exists(file.path(dataset_title))) dir.create(file.path(dataset_title))
if (!dir.exists(file.path(dataset_title,"output"))) dir.create(file.path(dataset_title,"output"))

# source all functions in folder 'mea-acute-neural-stats-to-mc0-scripts'
scripts <- list.files(path = "mea-acute-neural-stats-to-mc0-scripts", pattern = "\\.R$", full.names = T, recursive = F)
sapply(scripts, source)

# loading acsn_acnm map
acsn_map <- as.data.table(read.csv(file.path("neural_stats_acsn_to_tcpl_acnm_map.csv")))
# acsn_map <- acsn_map[, .(acsn, acnm)]
```

Save items to be kept throughout all levels
```{r}
keep.items <- c(ls(),'keep.items')
```


# Level 0 - Gather and Check Files

## Read any .txt files with important notes

Scan for readme's that might affect dosing, wllq

```{r}
txt.files <- list.files(start.dir, pattern = '\\.txt', recursive = T, full.names = T)
readmes <- txt.files[grepl('read( )*me',tolower(txt.files))]
if (length(readmes) > 0) {
  for (i in 1:length(readmes)) {
    cat(i,'\n')
    cat(dirname(readmes[i]),'\n')
    cat(scan(readmes[i], what = character(), sep = '\n', quiet = T), sep = '\n')
    cat('\n')
  }
} else {
  cat('no readme.txt files')
}
# 1 
# L:/Lab/NHEERL_MEA/Project TSCA 2019/Acute TSCA Conc Response/20210526 Culture G26 (201B07, 201C02, 201C03 may need repeat) 
# Plate 1 CellTiter Blue blank control is not valid
# blank values are taken from plate 2 blank controls
```

5/11/23: Seline has already updated the formula in the Calculations file such that the corrected optical density values for plate 1 are corrected to the blanks from plate 2. No further updates needed.

Check for any other .txt files that look relevant
```{r}
cat('other txt files:\n')
cat(setdiff(txt.files,readmes), sep = "\n")
# (if there are any other txt files that look like they might have relevant notes, review the contents of those files as well)
```

Update logs are just files I made to note updates to the Calculations files - these are fine.

## Get list culture folders to review

Get a list of all cultures in the main folder to check off as I review wllq notes in lab notebook and determine usability.

`eval = FALSE` because I only need to run this once

```{r eval = FALSE}
# Just want to get a list of all cultures in the main folder
group.folders <- list.files(path = start.dir, pattern = '[0-9]{8}', include.dirs = T)
cat(group.folders, sep ='\n')
wb <- createWorkbook()
addWorksheet(wb, 'TSCA cultures')
writeData(wb, 1, data.table('culture_folders' = group.folders))
saveWorkbook(wb, file = 'TSCA2019/tables/TSCA2019_culture_folders_to_review.xlsx')
```

## Get list of neural stats files, meta data files, and (opt) raw cytotoxicity data files

```{r}
cat(paste0(dataset_title, " MEA Acute TCPL Level 0 Data Prep Running Log\nDate: ",as.character.Date(Sys.Date()),"\n"))

# For each culture, get all files under "Neural Statistic Compiler" folder
# if there are not exactly 6 files per folder -> flag (including if there are just 0)
group.folders <- list.files(path = start.dir, 
                            pattern = '[0-9]{8}',
                            include.dirs = T,
                            full.names = T)
print(basename(group.folders))
# (edit the list of group folders to include if necessary)
```

Looks okay!

Select input files to use, store files in .txt file
```{r}
if (select.neural.stats.files) {
  
  # Use below lines of code to get all files in folder "Neural Statistic Compiler" for each group
  # Or, if project is not cleanly organized, use below function to cycle through 
  # folders in project and manually select files using the 'choose.files' window interface
  # selectInputFiles(start.dir, dataset_title, files_type = "neural_stats")
  
  # Get all neural stats files in folder
  neural.stats.files <- sapply(group.folders, 
                               function(folderi) list.files(path = file.path(folderi,
                                                                             'Neural Statistic Compiler'), 
                                                            pattern = '\\.csv', 
                                                            full.names = T,
                                                            recursive = T))
  
  # Check that there are exactly 6 neural stats files per group
  # (1 baseline and 1 treated X 3 plates)
  num.files.per.group <- lapply(neural.stats.files, length)
  cat('Groups that did not match exactly 6 neural stats files:')
  num.files.per.group[num.files.per.group != 6]
  
  # (edit the list of neural.stats.files if needed)
  
  # Exclude 20201127_MW71-7113 (because treated recording broken up), for now (will investigate Seline's methods to combine and if I agree if time later)
  # Similar situation for G24, plate 75-8213 
  neural.stats.files <- sapply(neural.stats.files,
                               function(files) files[!grepl('20201125_MW71-7113',files)])
  neural.stats.files <- sapply(neural.stats.files,
                               function(files) files[!grepl('20210512_MW75-8213',files)])
  num.files.per.group <- lapply(neural.stats.files, length)
  
  # Final check:
  # stopifnot(sum(num.files.per.group != 6) == 0)
  # (removing this check because there are 2 groups with 1 plate removed)
  # (so I'll just confirm that there are an even # of files per group)
  stopifnot(sum(unlist(num.files.per.group) %% 2) == 0)
  
  # Write files to log
  writeLogFile(unlist(neural.stats.files), dataset_title, files_type = 'neural_stats')
  
}
```

Select calculations files
```{r}
if (select.calculations.files) {
  
  # Use below lines of code to get files in group folders that contain the phrase "Calculations"
  # Or, if project is not cleanly organized, use below function to cycle through 
  # folders in project and manually select files using the 'choose.files' window interface
  # selectInputFiles(start.dir, dataset_title, files_type = "calculations")
  
  # Get all calculations files in folder
  calc.files <- sapply(group.folders, 
                       function(folderi) list.files(path = file.path(folderi), 
                                                    pattern = 'Calculations', 
                                                    full.names = T,
                                                    recursive = F))
  
  # Remove any dummy "ghost" files
  calc.files <- sapply(calc.files,
                       function(files) files[!grepl('\\~\\$',basename(files))])
  
  # Check that there is exactly 1 calc file per group
  num.files.per.group <- lapply(calc.files, length)
  cat('Groups that did not match exactly 1 Calculations file:')
  num.files.per.group[num.files.per.group != 1]
  
  # (edit the list of calc.files if needed)
  
  # Final check:
  stopifnot(sum(num.files.per.group != 1) == 0)
  
  # Write files to log
  writeLogFile(unlist(calc.files), dataset_title, files_type = 'calculations')
  
}
```

Select raw cytotoxicity data files

```{r}
if (select.raw.cytotox.files) {
  
  # Use below lines of code to get files in group folders that contain the phrase "Calculations"
  # Or, if project is not cleanly organized, use below function to cycle through 
  # folders in project and manually select files using the 'choose.files' window interface
  # selectInputFiles(start.dir, dataset_title, files_type = "calculations")
  
  # Get all cytotox files
  # (note that namesa re not retained with recursive = T)
  all.xls.files <- unlist(lapply(group.folders, 
                                 function(folderi) list.files(path = file.path(folderi), 
                                                              pattern = '\\.xls$', 
                                                              full.names = T, 
                                                              recursive = T)))
  raw.cytotox.files.ungrouped <- all.xls.files[grepl('Cytotoxicity',all.xls.files)]
  all.group.folders <- dirname(dirname(raw.cytotox.files.ungrouped))
  raw.cytotox.files <- split(raw.cytotox.files.ungrouped, f = all.group.folders)
  
  # Check that there is exactly 1 LDH file per group
  num.LDH.per.group <- sapply(raw.cytotox.files, function(seti) sum(grepl('LDH',seti)))
  cat('Groups that do not have exactly 1 LDH file:')
  num.LDH.per.group[num.LDH.per.group != 1]
  
  # (if any, edit selection or rename files if missing "LDH" in name)
  
  # Check that there are exactly 3 AB files per group
  num.AB.per.group <- sapply(raw.cytotox.files, function(seti) sum(!grepl('LDH',unlist(seti))))
  cat('Groups that do not have exactly 3 AB file:')
  num.AB.per.group[num.AB.per.group != 3]
  
  # Write files to log
  writeLogFile(unlist(raw.cytotox.files), dataset_title, files_type = 'raw_cytotox')
  
}
```


# Level 1 

## Read neural stats files

Extract all of the data from the files and transform into long data format (dat1)

```{r}
dat1 <- extractAllData(dataset_title, 
                       acsn_map,
                       append = F, 
                       plate.id.tag.location = NULL,
                       noisy_functions = noisy_functions)
```

Get plate.id from filename for files that don't have plate ID in file header (because were ran with older machine/software)

```{r}
dat1[is.na(plate.id) | plate.id %in% c('MW'), .N, by = .(neural_stats_file, plate.id)]
dat1[, check_plate_ids := is.na(plate.id) | plate.id %in% c('MW')]
dat1[, .N, by =.(setting_axis.version, check_plate_ids)] # all the plates that dont' have plate ids come from the same axis version
dat1[is.na(plate.id) | plate.id %in% c('MW'), plate.id := stri_extract(neural_stats_file, regex = 'MW[^_]+')] # note that sometimes the 
dat1[check_plate_ids == TRUE, .N, by = .(plate.id, neural_stats_file)]
```

Looks good!

## Determine run type for each file

Get a numeric, sortable version of the time from the original_start_time and experiment start time (to use to help determine which files are baseline versus treated).

```{r}
dat1 <- getNumericTimeValFromString(dat1, time_string = 'original_file_time', new_cols_suffix = 'oft')
dat1 <- getNumericTimeValFromString(dat1, time_string = 'experiment_start_time', new_cols_suffix = 'est')
```

Rank the files associated with each experiment.date - plate.id combination based on the experiment start time, original file time, and neural_stats_file (name). Ideally, each experiment.date - plate.id combination will correspond to 2 files, with 1 file clearly corresponding to the chronologically first "baseline" recording and the other the secondary "treated" recording. Ideally this ranking would be agree for all 3 methods.

```{r}
# Determine the run type based on the 2 times given in file header
dat1[, file_rank_oft := frank(time_posix_num_oft, ties.method = 'dense'), 
     by = .(experiment.date, plate.id)]
dat1[, file_rank_est := frank(time_posix_num_est, ties.method = 'dense'), 
     by = .(experiment.date, plate.id)]

# Determine ranking based on file name
dat1[, file_rank_name := frank(neural_stats_file, ties.method = 'dense'), 
     by = .(experiment.date, plate.id)]
```

Check for discrepancies in file rank from the 3 methods
```{r}
dat1[, .N, by = .(file_rank_oft, file_rank_name, file_rank_est)]
#   file_rank_est file_rank_oft file_rank_name      N
# 1:             1             1              1 228672
# 2:             2             2              2 222480
# 3:             1             2              2   6192

if (nrow(dat1[!(file_rank_est == file_rank_oft & file_rank_oft == file_rank_name)]) != 0) {
  warning('Rankings of files for each experiment date and plate.id pair by experiment start time, original file time, and file name do not all agree')
}
```

Investigate discrepancies
```{r}
dat1[file_rank_est != file_rank_oft, .N, by = .(experiment.date, plate.id, experiment_start_time, original_file_time)]
#    experiment.date  plate.id experiment_start_time  original_file_time    N
# 1:        20201208 MW71-7111   12/08/2020 10:32:25 12/08/2020 12:04:05 2064
# 2:        20201208 MW71-7112   12/08/2020 12:45:12 12/08/2020 14:10:03 2064
# 3:        20210202 MW72-8209   02/02/2021 12:46:36 02/02/2021 14:18:38 2064
dat1[, date_plate := paste0(experiment.date, '_',plate.id)]
check.dps <- dat1[file_rank_est != file_rank_oft, unique(date_plate)]
dat1[date_plate %in% check.dps, .N, by = .(neural_stats_file, file_rank_est, file_rank_oft, experiment_start_time, original_file_time)]
#                            neural_stats_file file_rank_est file_rank_oft experiment_start_time  original_file_time    N
# 1: AC_20201125_MW71-7111_13_00(001)(000).csv             1             1   12/08/2020 10:32:25 12/08/2020 10:52:26 2064
# 2: AC_20201125_MW71-7111_13_00(002)(000).csv             1             2   12/08/2020 10:32:25 12/08/2020 12:04:05 2064
# 3: AC_20201125_MW71-7112_13_00(000)(000).csv             1             1   12/08/2020 12:45:12 12/08/2020 13:05:13 2064
# 4: AC_20201125_MW71-7112_13_00(001)(000).csv             1             2   12/08/2020 12:45:12 12/08/2020 14:10:03 2064
# 5: AC_20210120_MW72-8209_13_00(000)(000).csv             1             1   02/02/2021 12:46:36 02/02/2021 13:08:34 2064
# 6: AC_20210120_MW72-8209_13_00(001)(000).csv             1             2   02/02/2021 12:46:36 02/02/2021 14:18:38 2064
```
Okay, so we have 3 plates where the experiment start time is the same in both the treated and recording files. I could probably just take the original file time and use that, especially since it agrees with the file_rank_name.

Usually, the original file time is about 20 minutes after the experiment start time. 

```{r}
dat1[, .N, by = .(experiment.date, plate.id, experiment_start_time, original_file_time, file_rank_name)][order(experiment.date, plate.id)]
```

Most likely, the experiment start time corresponds to when the plate was put in the machine for the baseline recording or when the plate was dosed for the treated (so perhaps the experiment start time resets every time with lid is opened? Then the original file time would correspond to when the recording actually starts.

So it appears that for these 3 plates (MW71-7111, MW71-7112, and MW72-8209), the experiment start time did not reset after the dosing (perhaps these plates were ran on a different machine that didn't require opening the lid?)

```{r}
dat1[, .N, by = .(setting_axis.version, ranks_agree = file_rank_est == file_rank_oft)]
```

Well, there isn't 1 axis version that explains this consistently. 

Regardless, I think we can rely on the ranking of the original file time and the file names to identify the treated and baseline recordings for these 3 plates.

Make final run_type determinations
```{r}
# Determine the file type - here I'm going to use the consensus of the file_rank_oft and the file_rank_name
dat1[file_rank_oft == 1 & file_rank_name == 1, run_type := 'baseline']
dat1[file_rank_oft == 2 & file_rank_name == 2, run_type := 'treated']
stopifnot(nrow(dat1[is.na(run_type)]) == 0)
```

Confirm there is exactly 1 baseline and 1 treated file per experiment/plate

```{r}
dat1[, .(num_base_files = length(unique(neural_stats_file[run_type == 'baseline'])),
         num_trt_files = length(unique(neural_stats_file[run_type == 'treated']))),
     by = .(experiment.date, plate.id)][num_base_files != 1 | num_trt_files != 1]
```



## Setting wllq_lvl1 based on run type

```{r}
dat1 <- level1_set_wllq(dat1, standard_analysis_duration_requirement)
```

Summarize output of wllq_lvl1 (taken from extract all data)

```{r}
dat1[, .(num_wells= length(unique(paste0(experiment.date,plate.id,well)))), by = .(wllq_lvl1, run_type, wllq_lvl1_notes)][order(run_type, -wllq_lvl1)]
```

Note that all treated run_types are expected to have wllq_lvl1 == NA at this point, unless analysis duration was outside the allowable range (then would have wllq_lvl1 == 0).


## Check consistency in settings in Neural Statistics Compiler header

```{r}
files <- read_files(dataset_title)
settings.list <- lapply(files, getNeuralStatsHeaderDat)
set.tb <- rbindlist(settings.list)
rm(settings.list)

# Remove rows that are not really settings and are already captured in dat1
set.tb <- set.tb[!setting %in% c('Original File Time','Experiment Start Time','Plate Serial Number')]
```

Check if all files have all the same setting types (informs which maestro version used)

```{r}
# Check if all files have the same setting types
set.tb[, .(setting_types = paste0(unique(setting_type),collapse = ",")), by = .(neural_stats_file)][, .N, by = .(setting_types)]
#                                                                                                                setting_types   N
# 1: Maestro Pro Settings,Digital Filter Settings,Spike Detector Settings,Burst Detector Settings,Statistics Compiler Settings 212
# 2:     Maestro Settings,Digital Filter Settings,Spike Detector Settings,Burst Detector Settings,Statistics Compiler Settings   6
```
ah - 212 files have "Maestro Pro Settings", 6 files have "Maestro Settings". 
Probably because these 6 used a different Maestro was used (original maestro vs Maestro Pro)
```{r}
set.tb[, maestro_type := unique(sub(' Settings','',setting_type[grepl('Maestro',setting_type)])), by = .(neural_stats_file)]
set.tb[, .N, by = .(maestro_type)]
# My guess is that these 6 files have settings that are common for that maestro, but not the Maestro Pro
```

Confirm that all settings within a given pair of treated and baseline recordings is consistent

```{r}
# merge in run_type determination by file name
set.tb <- merge(set.tb, dat1[, unique(.SD), .SDcols = c('neural_stats_file','run_type','experiment.date','plate.id')], by = 'neural_stats_file', all = T)

# Check for uniqueness
set.tb[, num_unique_by_exp_plate := length(unique(setting_val)), 
       by = .(maestro_type, setting_type, setting, experiment.date, plate.id)]
set.tb[num_unique_by_exp_plate != 1,  .N, by = .(maestro_type, setting_type, setting)]
#    maestro_type         setting_type             setting  N
# 1:  Maestro Pro Maestro Pro Settings CO2 Conc. Set Point  4
# 2:  Maestro Pro Maestro Pro Settings    Actual CO2 Conc. 24
# 3:  Maestro Pro Maestro Pro Settings Current Temperature  6

# View variability in these cases
set.tb[num_unique_by_exp_plate != 1 & !setting %in% c('Original File Time','Experiment Start Time'),  .N, by = .(maestro_type, setting_type, setting, setting_val)][order(maestro_type, setting_type, setting, setting_val)]
```

None of these look truly concerning.


Check for setting_types with inconsistent setting values.

```{r}
set.tb[, num_unique_vals := length(unique(setting_val)),
       by = .(setting_type, setting)]
set.tb[num_unique_vals != 1, 
       .(maestro_types = paste0(unique(maestro_type),collapse = ",")),
       by = .(setting_type, setting, num_unique_vals)]
#                     setting_type                                   setting num_unique_vals       maestro_types
#  1:         Maestro Pro Settings                     Temperature Set Point               2         Maestro Pro
#  2:         Maestro Pro Settings                       Current Temperature               4         Maestro Pro
#  3:         Maestro Pro Settings                       CO2 Conc. Set Point               2         Maestro Pro
#  4:         Maestro Pro Settings                          Actual CO2 Conc.               6         Maestro Pro
#  5:         Maestro Pro Settings                              AxIS Version               3         Maestro Pro
#  6:         Maestro Pro Settings                      Maestro Pro Firmware               3         Maestro Pro
#  7:      Digital Filter Settings                     Low Pass Cutoff Freq.               2 Maestro Pro,Maestro
#  8:      Spike Detector Settings                                 Threshold               2 Maestro Pro,Maestro
#  9:      Burst Detector Settings Minimum Number of Spikes (network bursts)               2 Maestro Pro,Maestro
# 10: Statistics Compiler Settings                       Include Source Data               2 Maestro Pro,Maestro
```

I see that the maestro type is contributing to the variability in some settings. I feel comfortable assuming that whatever settings are common for the older maestro were used for these 6 files. So I will check that the settings are internally consistent for each maestro type. Later, I will do a bigger analysis to confirm that data from the Maestro and Maestro Pro are comparable.

Check for maestro_type - setting_type combinations with inconsistent setting values.

```{r}
set.tb[, num_unique_vals := length(unique(setting_val)),
       by = .(maestro_type, setting_type, setting)]
set.tb[num_unique_vals != 1, 
       .N,
       by = .(maestro_type, setting_type, setting, num_unique_vals)]
#            setting_type               setting num_unique_vals   N
# 1: Maestro Pro Settings Temperature Set Point               2 212
# 2: Maestro Pro Settings   Current Temperature               4 212
# 3: Maestro Pro Settings   CO2 Conc. Set Point               2 212
# 4: Maestro Pro Settings      Actual CO2 Conc.               6 212
# 5: Maestro Pro Settings          AxIS Version               3 212
# 6: Maestro Pro Settings  Maestro Pro Firmware               3 212
```

**Investigate each case:**

* Temperature Set Point

```{r}
# Temperature Set Point
set.tb[num_unique_vals != 1 & setting == 'Temperature Set Point', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
# Looks like the temperature was intentionally changed at some point..
set.tb[, culture.date := stri_extract(neural_stats_file, regex = '[0-9]{8}')]
set.tb[setting == 'Temperature Set Point', .(.N,num_files = length(unique(neural_stats_file))), by = .(culture.date, setting_val)][order(culture.date)]
```
Looks like they intentionally switched from 37 C to 35 C starting on 20210512
(then switched back to 37 C for the last culture). 
I am making a note to look into this more once I have all data, to see how much temp affects controls, and if data is combinable

* Current Temperature

```{r}
# Current Temperature
set.tb[num_unique_vals != 1 & setting == 'Current Temperature', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```
Variability within +/-0.1 C is not a concern. Larger differences most likely correspond to variable temperature set points, addressed above.

* CO2 Conc. Set Point

```{r}
set.tb[num_unique_vals != 1 & setting == 'CO2 Conc. Set Point', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```
This is fine

* Actual CO2 Conc.

```{r}
set.tb[num_unique_vals != 1 & setting == 'Actual CO2 Conc.', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```
Again, this variability is fine, and I will check on the plate that did not have Co2

* AxIS Version

```{r}
set.tb[num_unique_vals != 1 & setting == 'AxIS Version', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```
I know that we have used variables AxIS Versions to date, so we have to live with it.

* Maestro Pro Firmware

```{r}
set.tb[num_unique_vals != 1 & setting == 'Maestro Pro Firmware', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```

Making a note to follow up.

Save set.tb for follow-up analyses

```{r}
setkey(set.tb, NULL)
set.tb.info <- paste0('Table containing the settings extracted from the Neural Statistics Compiler file headers. Made in run_me_TSCA2019.Rmd. Saved on ',Sys.Date())
save(set.tb, set.tb.info, file = file.path(dataset_title,'output','neural_stats_settings_table.RData'))
```


## Check analysis duration

Check that the analysis duration does not vary by more than 1% from the target (40 min, 2400 seconds)

```{r}
stopifnot(nrow(dat1[abs((analysis_duration - 2400)/2400) > 0.01]) == 0)
```

Any weirdness in the analysis start?

```{r}
dat1[, .N, by = .(analysis_start)]
```

All starting at 0, this is fine.

## Determine culture.date & group

Can get culture.date from the file name itself or from the file foldername. I'm finding that the foldername tends to be a bit more reliable (less prone to typos).

```{r}
# Get full file names to get culture date from 
files <- read_files(dataset_title)
files.tb <- data.table(fullname = files)
files.tb[, basename := basename(fullname)]
files.tb[, culture_folder := basename(dirname(dirname(fullname)))]
files.tb[, group_char := stri_extract(culture_folder, regex = 'G[0-9]+')]
files.tb[, group_int := stri_extract(group_char, regex = '[0-9]+')]
files.tb[, culture.date := stri_extract(culture_folder, regex = '[0-9]{8}')]
files.tb[, .N, by = .(culture.date)]
dat1 <- merge(dat1, files.tb, by.x = 'neural_stats_file', by.y = 'basename', all.x = T)

# Confirm days from culture to experiment date are within 13-15
# (if not, either there is a typo, or need to consider if data is similar enough to other expects performed in DIV 13-15)
dat1[, days_to_exp := as.numeric(as.Date(experiment.date, format = '%Y%m%d'))
     - as.numeric(as.Date(culture.date, format = '%Y%m%d'))]
dat1[, .(num_files= length(unique(neural_stats_file))), by = .(days_to_exp)][order(-num_files)]
#    days_to_exp  V1
# 1:          13 110
# 2:          15 102
# 3:          14   6
# looks good/reasonable!

# Compare the culture date from foldername to culture date in file name
# Resolve discrepancies
dat1[, culture.date.file := stri_extract(neural_stats_file, regex = '[0-9]{8}')]
dat1[culture.date != culture.date.file, .(length(unique(neural_stats_file))), by = .(culture_folder, culture.date.folder = culture.date, culture.date.file, experiment.date, days_to_exp)]
#                                   culture_folder culture.date.folder culture.date.file experiment.date days_to_exp V1
# 1:                          20210310 Culture G13            20210310          20210317        20210325          15  4
# 2: 20210324 Culture G14 (199E02 may need repeat)            20210324          20210331        20210406          13  6
# 3: 20210414 Culture G19 (200A08 may need repeat)            20210414          20210429        20210429          15  6
# 4: 20210602 Culture G28 (201E01, 201F04, 201F05)            20210602          20210526        20210615          13  6
# 5:                          20210602 Culture G29            20210602          20210526        20210616          14  6

```


G13 - lab notebook clearly indicates that culture.date was 2021-03-10 for all plates. I'm faily confident that the culture.date.file was just a typo
G14 - again, lab notebook agrees with culture date from folder name
G19 - again, lab notebook agrees with culture date from folder name. Looks like culture.date.file got changed to experiment.date
G28 - again, lab notebook agrees with culture date from folder name. 20210526 was the culture date of the previous culture, so likely the files names just go copied over.
G29 - again, lab notebook agrees with culture date from folder name. 20210526 was the culture date of the previous culture, so likely the files names just go copied over.

So in every cases, the culture date in the folder name agrees with the lab notebook. I'm going to assume that this is correct, and that the culture.date form the file names that are different are typos.

```{r}
# finalize the culture.date, get rid of extra columns
# culture.date (from folder) does not need any changes
dat1[, culture.date.file := NULL]
```

## Save updated dat1

```{r}
setkey(dat1, NULL)
save(dat1, file = paste0(dataset_title, "/output/",dataset_title,"_dat1.RData"))
rm(list = setdiff(ls(), keep.items))
```


# Level 2 

## Calculate % change in activity values

```{r}
# collapse the plate data by calculating the percent change in activity (dat2)
dat2 <- calcActivityPercentChange(dataset_title)

# check it out
dat2[wllq_lvl2==1, summary(rval)]
```

Minimum rval should be approximately -100 (slightly below -100 is normal), max is usually Inf.

## Save updated dat2

```{r}
setkey(dat2, NULL)
save(dat2, file = file.path(dataset_title,'output',paste0(dataset_title,"_dat2.RData")))
rm(list = setdiff(ls(), keep.items))
```

# Level 3

## Read in raw cytotox values

Get list of cytotox files

```{r}
raw.cytotox.files <- read_files(dataset_title, files_type = 'raw_cytotox')
```


Note assumptions for LDH and AB raw data file reading functions:

**LDH**

The function `read_acute_LDH_raw_data` assumes that:

* All plates are 96-well (8x12)
* For every plate, column 1, there is a non-NA entry that contains the plate info (e.g., "20201104_20201117_MW71-7104_TSCA Acute DR_Plate 1")
* For every plate, in column 2, there is a cell that contains the word "Temperature"
* Raw data values for each plate start 1 row below every occurrence of the word "Temperature" and continue for the next 7 rows
* Raw data values for all plates are in columns 3 - 14

**AB**:

The function `read_acute_AlamarBlue_raw_data` assumes that:
* 1 plate per file
* Plates are 96-well (8x12)
* There is a cell with the letter "A" in the first column that signifies the first row of the plate
* Raw data values start in the same row as the letter "A" in column 1 and continue for the next 7 rows
* Raw data values are in columns B:M (i.e., 2 - 13)


There are some checks in the functions to see if each file passes these assumptions. If a fail fails these checks, the function will print a statement with a description of the fail. But, a file could have some unexpected data alignment that is not caught by the checks. In that case, thus, these functions are not 100% reliable to read in the raw data, and the values should be compared to the Calculations files.

If a file fails the checks, just manually confirm that the raw data values for those files have been correctly copy-pasted into the Calculations file.


Read raw data from LDH files
```{r}
raw.cytotox.files.LDH <- grep('LDH',raw.cytotox.files, val = T)
rawdat.LDH <- data.table()
for (filei in raw.cytotox.files.LDH) {
  add.tb <- read_acute_LDH_raw_data(filei)
  rawdat.LDH <- rbind(rawdat.LDH, add.tb)
}

# # To debug an individual file:
# filei
# debugonce(read_acute_LDH_raw_data)
# add.tb <- read_acute_LDH_raw_data(filei)

# Note any LDH files that could not be read (manually check that these data values have been correctly copied over to the Calculations file)
setdiff(basename(raw.cytotox.files.LDH), rawdat.LDH$filename)
```

Read raw data from AB files
```{r}
raw.cytotox.files.AB <- raw.cytotox.files[!grepl('LDH',raw.cytotox.files)]
rawdat.AB <- data.table()
for (filei in raw.cytotox.files.AB) {
  add.tb <- read_acute_AlamarBlue_raw_data(filei)
  rawdat.AB <- rbind(rawdat.AB, add.tb)
}

# # To debug an individual file:
# filei
# debugonce(read_acute_AlamarBlue_data)
# add.tb <- read_acute_AlamarBlue_data(filei)

# Note any AB files that could not be read (manually check that these data values have been correctly copied over to the Calculations file)
setdiff(basename(raw.cytotox.files.AB), rawdat.AB$filename)
```

Confirm expected # of raw data values per file

```{r}
rawdat.AB[, .N, by =.(filename)][N != 96]
rawdat.LDH[, .N, by =.(filename)][N != 96*3]
```


Get plate IDs for raw AB and LDH data, then combine

```{r}
# LDH

# Get plate.id from plate.info (taken from column A of each file) as:
# (1-2 digits)-(3-4 digits)
rawdat.LDH[, plate.id := stri_extract(plate.info, regex = '[0-9]{1,2}\\-[0-9]{3,4}')]
rawdat.LDH[, plate.id := paste0('MW',plate.id)]
rawdat.LDH[, .N, by = .(plate.id)] # confirm these looks correct

# AB

# Get plate.id from filename as:
# (1-2 digits)-(3-4 digits)
rawdat.AB[, plate.id := stri_extract(filename, regex = '[0-9]{1,2}\\-[0-9]{3,4}')]
rawdat.AB[, plate.id := paste0('MW',plate.id)]
rawdat.AB[, .N, by = .(plate.id)] # confirm these looks correct


# Combine cyto data
rawdat.cytotox <- rbind(rawdat.LDH, rawdat.AB, fill = T)
```

Get the culture date from the folders for raw cytotox data

```{r}
# Get full file names to get culture date from 
files <- read_files(dataset_title, files = 'raw_cytotox')
files.tb <- data.table(fullname = raw.cytotox.files)
files.tb[, basename := basename(fullname)]
files.tb[, culture_folder := basename(dirname(dirname(fullname)))]
files.tb[, group_char := stri_extract(culture_folder, regex = 'G[0-9]+')]
files.tb[, group_int := stri_extract(group_char, regex = '[0-9]+')]
files.tb[, culture.date := stri_extract(culture_folder, regex = '[0-9]{8}')]
files.tb[, .N, by = .(culture.date)]
rawdat.cytotox <- merge(rawdat.cytotox, files.tb, 
                        by.x = 'filename', by.y = 'basename', all.x = T)

# Confirm 3 unique plates of each assay per group
rawdat.cytotox[, .(num_unique_plate_ids = length(unique(plate.id))), by = .(acsn, culture_folder, group_char)][num_unique_plate_ids != 3]
# (if any cases, could be that there was a typo in the plate.id in the filename for AB or in the file header for LDH. Check it out and correct as needed)
```

Add the official acnm's

```{r}
rawdat.cytotox <- merge(rawdat.cytotox, acsn_map[, .(acsn, acnm)], by = 'acsn', all.x = T)
```


## Re-orient plates as needed

* For G11, reverse the plate orientation for AB Plate 71-7015 (not implemented in raw or calc file) *** (flip 180 degrees. Well A8 -> F1)
* For G20 plate 75-8201, reverse the plate orientation for AB (not implemented in raw or calc file) *** (flip 180 degrees. Well A8 -> F1)

```{r}
# confirm all rows and cols are defined
rawdat.cytotox[, .N, by = .(is.na(coli), is.na(rowi))]
rawdat.cytotox[, `:=`(rowi = as.integer(rowi), coli = as.integer(coli))]
rawdat.cytotox[, well_org := paste0(LETTERS[rowi],coli)]

# Save copy of current row & col
rawdat.cytotox[, `:=`(rowi_org  = rowi, coli_org = coli)]

# Switch the rows and columns for the upper left 48 wells
rawdat.cytotox[group_int == '11' & plate.id == 'MW71-7015' & acsn == 'AB' 
               & rowi %in% c(1:6) & coli %in% c(1:8), 
               `:=`(rowi = 7 - rowi_org,
                    coli = 9 - coli_org)]
rawdat.cytotox[group_int == '20' & plate.id == 'MW75-8201' & acsn == 'AB'
               & rowi %in% c(1:6) & coli %in% c(1:8), 
               `:=`(rowi = 7 - rowi_org,
                    coli = 9 - coli_org)]
```

Visually confirm G11 MW71-7015 rotation looks correct

```{r}
test.plate <- dcast(rawdat.cytotox[group_int == '11' & plate.id == 'MW71-7015' & acsn == 'AB'], rowi ~ coli, value.var = 'well_org')
test.plate
```


Since the calculations files have not already been corrected to reflect the rotation (since this is difficult to do in Excel), I will create .csv files with the raw values that have been rotated, then manually copy-paste in the Calculations files.


```{r}
# Group 11, plate 7015
fullname.org <- rawdat.cytotox[group_int == '11' & plate.id == 'MW71-7015' & acsn == 'AB', unique(fullname)]
fullname.adj <- sub('\\.xls','_upper_48well_raw_vals_rotated_180degrees.csv',fullname.org)
tb1 <- dcast(rawdat.cytotox[group_int == '11' & plate.id == 'MW71-7015' & acsn == 'AB'], 
             rowi ~ coli, value.var = 'rval_not_blank_corrected')
tb1[, row_char := LETTERS[rowi]]
write.csv(tb1[, c('row_char', 1:12)], row.names = F, file = fullname.adj)

# Group 20, plate MW75-8201
fullname.org <- rawdat.cytotox[group_int == '20' & plate.id == 'MW75-8201' & acsn == 'AB', unique(fullname)]
fullname.adj <- sub('\\.xls','_upper_48well_raw_vals_rotated_180degrees.csv',fullname.org)
tb1 <- dcast(rawdat.cytotox[group_int == '20' & plate.id == 'MW75-8201' & acsn == 'AB'], 
             rowi ~ coli, value.var = 'rval_not_blank_corrected')
tb1[, row_char := LETTERS[rowi]]
write.csv(tb1[, c('row_char', 1:12)], row.names = F, file = fullname.adj)

```


## Calculate blank-corrected values

Calculate average of blank wells. Blank wells in both assays are in G1, 2 and 3

```{r}
# check that there are 3 non-NA blanks per plate
stopifnot(nrow(rawdat.cytotox[, .(sum(rowi == 7 & coli %in% c(1:3) & !is.na(rval_not_blank_corrected))), by = .(culture.date, plate.id, acsn)][V1 != 3]) == 0)

rawdat.cytotox[, blank_well_avg := mean(rval_not_blank_corrected[rowi == 7 & coli %in% c(1:3)], na.rm = T),
               by = .(culture.date, plate.id, acsn)]

```


(Manually check well quality tables to see if any blank wells have wllq == 0).

-> Culture 20210414 plate 75-8114 LDH well G3 has wllq == 0, so will exclude that well from blank well calculation

```{r}
rawdat.cytotox[culture.date == '20210414' & plate.id == 'MW75-8114' & acsn == 'LDH' & rowi %in% c(7) & coli %in% 1:3, .(rowi, coli, rval_not_blank_corrected)]
rawdat.cytotox[culture.date == '20210414' & plate.id == 'MW75-8114' & acsn == 'LDH',  
               blank_well_avg := mean(rval_not_blank_corrected[rowi == 7 & coli %in% c(1:2)], na.rm = T),
               by = .(culture.date, plate.id, acsn)]
```


Subtract blank wells from all other wells

```{r}
rawdat.cytotox[, rval := rval_not_blank_corrected - blank_well_avg]
```


## Read in cytotox data the traditional way & compare


Confirm output looks okay,

Then merge with raw data, making adjustments as needed as noted in OneNote

```{r}
# get cytotox data
cytodat <- getAllCytoData(dataset_title)
# some values are negative (-1081 - -3.3333e-05):
#                         acnm rval_is_neg    N
# 1:  CCTE_Shafer_MEA_acute_AB       FALSE 5277
# 2: CCTE_Shafer_MEA_acute_LDH        TRUE 2566
# 3: CCTE_Shafer_MEA_acute_LDH       FALSE 3649
# 4:  CCTE_Shafer_MEA_acute_AB        TRUE   51
# These will be set to 0
# 
# cytodat is ready
# Warning message:
# In valuesUnderTagPhrase(assay_dat_dt, tagPhrase = value_tagPhrase,  : 
#  Some LDH rval on 75-8114 are NA (may include some cells that are not needed for final cytodat)

```


If there are any warnings about NA raw values
* confirm that these are expected (rather than e.g. unexpected data alignment)

I know that some of the lysis wells were not usable on MW75-8114e - were any other wells NA?

```{r}
cytodat[plate.id == 'MW75-8114' & is.na(rval)] 
```

All is okay.

Get culture dates

```{r}
# Get full file names to get culture date from 
files <- read_files(dataset_title, files = 'calculations')
files.tb <- data.table(fullname = files)
files.tb[, basename := basename(fullname)]
files.tb[, culture_folder := basename(dirname(fullname))]
files.tb[, group_char := stri_extract(culture_folder, regex = 'G[0-9]+')]
files.tb[, group_int := stri_extract(group_char, regex = '[0-9]+')]
files.tb[, culture.date := stri_extract(culture_folder, regex = '[0-9]{8}')]
files.tb[, .N, by = .(culture.date)]
cytodat <- merge(cytodat, files.tb, 
                 by.x = 'srcf', by.y = 'basename', all.x = T)

# Confirm 3 unique plates of each assay per group
cytodat[, .(num_unique_plate_ids = length(unique(plate.id))), by = .(acnm, culture_folder, group_char)][num_unique_plate_ids != 3]
# (if any cases, could be that there was a typo in the plate.id in the filename for AB or in the file header for LDH. Check it out and correct as needed)
```


## Compare with raw cytotoxicity data, resolve discrepancies

```{r}
# Which rows to merge? (rawdat will contain some empty wells that are not needed)
cytodat[grepl('AB',acnm), .N, by = .(rowi, coli)]

# Subset rawdat.cytotox to wells we actually want to use
rawdat.cytotox[, use_wells := 0]
rawdat.cytotox[acsn == 'AB' & rowi %in% c(1:6) & coli %in% c(1:8), use_wells:=1]
rawdat.cytotox[acsn == 'LDH' & ((rowi %in% c(1:6) & coli %in% c(1:8)) |
                                  (rowi == 7 & coli %in% c(4,5)) |
                                  (rowi == 8 & coli %in% c(1:6))), use_wells := 1]
rawdat.cytotox.tomerge <- rawdat.cytotox[use_wells == 1]

# Prepare to merge
cat(intersect(names(cytodat), names(rawdat.cytotox.tomerge)), sep = '", "')
cytodat[, in_calc := 1]
rawdat.cytotox.tomerge[, in_raw := 1]
nrow(cytodat)
nrow(rawdat.cytotox.tomerge)
comp.dat <- merge(cytodat, rawdat.cytotox.tomerge, by = c("plate.id", "rowi", "coli", "acnm",  "culture_folder", "group_char", "group_int", "culture.date"),
                  suffixies = c('.calc','.raw'), all = T)
```

### Test data presence agreement
```{r}
comp.dat[, .N, by = .(in_calc, in_raw)]
#    in_calc in_raw     N
# 1:       1      1 11335
# 2:      NA      1   209
# 3:       1     NA   208

setdiff(cytodat$culture.date, rawdat.cytotox$culture.date) # empty
setdiff(rawdat.cytotox$culture.date, cytodat$culture.date) # empty

setdiff(cytodat$plate.id, rawdat.cytotox$plate.id) # "MW75-7519" "MW75-9201"
setdiff(rawdat.cytotox$plate.id, cytodat$plate.id) # "MW75-5819" "MW75-9121"

comp.dat[plate.id %in% c("MW75-7519","MW75-9201",
                         "MW75-5819", "MW75-9121"), .N, by  = .(culture.date, plate.id, in_calc, in_raw, srcf, filename, acnm)][order(culture.date)]

```

For G12, in the lab notebook, plate 1 was originally written as 75-7519. Then it was crossed out and replaced with 75-5819 on 3/23/21 and initialed by Theresa. Therefore, I think 75-7519 in the Calculations file was a typo. Will update that here and in the Calculations file now.

```{r}
cytodat[culture.date == '20210310' & group_int == 12 & plate.id == 'MW75-7519', plate.id := 'MW75-5819']
```

For G36 - lab notebook and neural stats compiler files indicate that the plate.id should be 75-9201, not 75-9121, so I think MW75-9201 is correct.

```{r}
rawdat.cytotox.tomerge[group_int == 36 & plate.id == 'MW75-9121', plate.id := 'MW75-9201']
```

Re-merge

```{r}
comp.dat <- merge(cytodat, rawdat.cytotox.tomerge, by = c("plate.id", "rowi", "coli", "acnm",  "culture_folder", "group_char", "group_int", "culture.date"),
                  suffixies = c('.calc','.raw'), all = T)

# Test data presence agreement
comp.dat[, .N, by = .(in_calc, in_raw)]
#    in_calc in_raw     N
# 1:       1      1 11543
# 2:      NA      1     1

comp.dat[is.na(in_calc), .(acnm, culture.date, plate.id, rowi, coli, rval.x, rval.y)]
#                         acnm culture.date  plate.id rowi coli rval.x    rval.y
# 1: CCTE_Shafer_MEA_acute_LDH     20210414 MW75-8114    8    6     NA 0.8766333

```
Ah, yes, this is the plate where there were only two 1/2 lysis control wells instead of the usual 3. Wllq is 0 for this well regardless, so no updates are needed.

### Compare rvals

```{r}
comp.dat[, rval_pct_diff := abs(rval.x - rval.y)/((rval.x + rval.y)*0.5)*100]
comp.dat[rval_pct_diff >= 0.5, 
         .(.N, affected = paste0(unique(acsn),collapse = ","), 
           max_pct_diff = max(rval_pct_diff), 
           max_raw_diff = max(abs(rval.x - rval.y))), by = .(group_int, culture.date, plate.id)][order(culture.date, group_int)]
# output as of 6/5/2023 at 6:17pm:
#     group_int culture.date  plate.id  N affected max_pct_diff max_raw_diff
#  1:         1     20201104 MW71-7106 17      LDH  2955.811277 8.562000e-01
#  2:         6     20210120 MW72-8208 18      LDH  1199.556541 9.918333e-01
#  3:         7     20210120 MW72-8212 20      LDH  3424.811219 1.058267e+00
#  4:        13     20210310 MW75-8003  1      LDH   200.000000 2.775558e-17
#  5:        20     20210428 MW75-8201 19      LDH   200.000000 4.863333e-01
#  6:        20     20210428 MW75-8202 64   AB,LDH   319.130435 7.477667e+03
#  7:        20     20210428 MW75-8203 62   AB,LDH   200.000000 6.216667e+03
#  8:        21     20210428 MW75-8204 66   AB,LDH   200.000000 1.368600e+04
#  9:        21     20210428 MW75-8205 65   AB,LDH   200.000000 1.244133e+04
# 10:        21     20210428 MW75-8206 68   AB,LDH   200.000000 1.301900e+04
# 11:        26     20210526 MW75-8219 47       AB     4.042701 9.776667e+02
# 12:        31     20210630 MW78-4406 48       AB   200.000000 1.235033e+04

```

**Review all discrepancies**

* Group 1 - The blank wells were abnormally high for Group 1, plate 3 - therefore, the blanks from plate 2 were used instead (2/3 blanks were ~10X those in plate 2). I agree with this decision.
* Group 6 - similiar situation, average of blanks taken from plate 2
* G7 - blanks for plate 2 taken from plate 1
* G13 - this is fine, actually difference is negligible (% diff just seems large because values are close to 0).
* G20 - I confirmed with Seline 5/9/23 that she just forget to copy over the raw data values into the Calcuations file for this group . I will update that Calculations file now. (Note that for plate 1, AB, raw values already updated when we updated due to reversed plate orientation).
* G21- again, I confirmed with Seline 5/9/23 that she just forget to copy over the raw data values into the Calcuations file for this group . I will update that Calculations file now.
* G26, AB - blanks taken from plate 2
* G31 - raw data for plate 3 in calculations file matches data for plate 3 in Group 30. So this portion of the excel spreadsheet probably just go missed.

Read in data from calculations files again to confirm updates

```{r}
cytodat <- getAllCytoData(dataset_title)
cytodat[, in_calc := 1]

# Get culture dates
cytodat <- merge(cytodat, files.tb, 
                 by.x = 'srcf', by.y = 'basename', all.x = T)

# Confirm 3 unique plates of each assay per group
cytodat[, .(num_unique_plate_ids = length(unique(plate.id))), by = .(acnm, culture_folder, group_char)][num_unique_plate_ids != 3]

# re-merge
comp.dat <- merge(cytodat, rawdat.cytotox.tomerge, by = c("plate.id", "rowi", "coli", "acnm",  "culture_folder", "group_char", "group_int", "culture.date"),
                  suffixies = c('.calc','.raw'), all = T)

# Confirm data presence alignment
comp.dat[, .N, by = .(in_calc, in_raw)]
comp.dat[is.na(in_calc), .(acnm, culture.date, plate.id, rowi, coli, rval.x, rval.y)]
#                         acnm culture.date  plate.id rowi coli rval.x    rval.y
# 1: CCTE_Shafer_MEA_acute_LDH     20210414 MW75-8114    8    6     NA 0.8766333
# wllq is 0 here, so no updates needed

# Compare rvals
comp.dat[, rval_pct_diff := abs(rval.x - rval.y)/((rval.x + rval.y)*0.5)*100]
comp.dat[rval_pct_diff >= 0.5, 
         .(.N, affected = paste0(unique(acsn),collapse = ","), 
           max_pct_diff = max(rval_pct_diff), 
           max_raw_diff = max(abs(rval.x - rval.y))), by = .(group_int, culture.date, plate.id)][order(culture.date, group_int)]
#    group_int culture.date  plate.id  N affected max_pct_diff max_raw_diff
# 1:         1     20201104 MW71-7106 17      LDH  2955.811277 8.562000e-01
# 2:         6     20210120 MW72-8208 18      LDH  1199.556541 9.918333e-01
# 3:         7     20210120 MW72-8212 20      LDH  3424.811219 1.058267e+00
# 4:        13     20210310 MW75-8003  1      LDH   200.000000 2.775558e-17
# 5:        26     20210526 MW75-8219 47       AB     4.042701 9.776667e+02
# 6:        31     20210630 MW78-4406 48       AB   200.000000 1.235033e+04
```

Good, now the only discrepancies correspond to the cases where a different plate was used to perform the blank-correction.


So there are some pieces of data that I would have got wrong if I had only looked at the calc files (i.e., where the raw data wasn't copied), and there are other pieces of data i would have missed if without the calc files (i.e., whether to use the blanks from another plate.)

To do:
* Summarize canonical course of action in instructions (i.e., read in both files, then compare, looking out for X; ultimately want calc files to be correct if possible and will plan to ultimately read data from those)
* Review what you've done here, condense where possible/helpful
* Officially have name cytotox data table, rm others
* Hmm, that seems like a lot of negatives/zero's for the LDH... is this right? Is this an indication of badness fro the assay? We'll check it out (but don't spend too much time - consider o items and where this woudl fit in importance-wise)
* See onenote for next steps!


SIMPLEST approach: (old)

* Take the raw cytotox data
* Get the meta data from the calc files
* Look at distribution of lysis control wells (need to develop a cutoff, as Kathleen said 5/31). Should I read in percent of control to discern which control well she usually uses? Did Seline even look at that? M Communicate with Kathleen that I haven't been usign her stuff up to this point... is that okay? (probs triage for later)



```{r}

# combine the cytodat with dat2, add trt, conc, and wllq to ea (dat3)
combineNeuralAndCyto(cytodat, main.output.dir, dataset_title)
# OUTPUT --------------------------------------------------------- 
# 
# ---------------------------------------------------------------- 
rm(cytodat)

# load dat3 and finalize it
cat("\n\nLevel 4 - Finalize well ID information:\n")
dat4 <- get_latest_dat(lvl = "dat3", dataset_title)
dat4[, dat2 := NULL]
dat4[, dat3 := basename(RData_files_used)]


# FINALIZE WLLQ
cat("\nFinalize Wllq:")
# set wllq to zero where rval is NA
cat("\nNA rval's:",dat4[wllq==1 & is.na(rval),.N])
#
cat("\nInf rval's (baseline==0):",dat4[wllq==1 & is.infinite(rval),.N])
# 
dat4[is.na(rval), `:=` (wllq = 0, wllq_notes = paste0(wllq_notes, "rval is NA; "))]
dat4[is.infinite(rval), `:=` (wllq = 0, wllq_notes = paste0(wllq_notes, "rval is Inf; "))]
cat("\nWell quality set to 0 for these rval's.\n")

# do any other updates to wllq based on notes from lab notebook
# e.g. misdosed, recording too long, etc.
# for example, updateWllq(dat4, date = "20190530", plate = "MW68-0807", well = "C6", wllq_note = "Contamination", override_check = override_wllq_checks)

# start a pdf to save the summary graphs
graphics.off()
pdf(file = file.path(main.output.dir, paste0(dataset_title, "_summary_figures_report_",as.character.Date(Sys.Date()),".pdf")), width = 10, height = 8)

# VERIFY TREATMENT LABELS FOR CONTROLS IN NEURAL AND CYTOTOX ASSAYS

cat("\nVerifying control compound labels:\n")
# view and standardize treatment names, so can compare all relevant values below
dat4[, .N, by = "treatment"]
dat4[grepl("DMSO",treatment), treatment := "DMSO"]

# visually confirm if the PICRO, TTX, LYSIS were added before the second recording for MEA endpoints
# varies across experiments, sometimes across days
# if not, the PICRO, TTX, LYSIS wells only contained media for the MEA endpoints
plotdat <- dat4[treatment %in% c("DMSO","PICRO","TTX","BIC","Media","Lysis","? Lysis","1:250 LDH","1:2500 LDH") & acnm == "CCTE_Shafer_MEA_acute_firing_rate_mean"]
view_activity_stripchart(plotdat, title_additions = "No Changes to Treatment Labels")
# RESPONSE:
# yes/no, it appears that the PICRO, TTX, LYSIS were added before the second treatment
# rename the treatment in the wells as needed

# for cytotoxicity assays, the "Media" wells at F1 should contain the LYSIS. Re-label the treatments to refect this

# for Cell Titer Blue assay:
plotdat <- dat4[treatment %in% c("DMSO","PICRO","TTX","BIC","Media","Lysis","? Lysis","1:250 LDH","1:2500 LDH") & grepl("(AB)",acnm)]
view_activity_stripchart(plotdat, title_additions = "No Changes to Treatment Labels")
# make updates if needed
# dat4[, AB.trt.finalized := FALSE] # set this to TRUE for individual plates as you update as needed
# 
# # for every other culture, the "Media" well in F1 contains Lysis at the time of the AB reading (or could change by well F1 vs by the name "Media"...)
# dat4[AB.trt.finalized == FALSE & grepl("AB",acnm) & treatment == "Media", .(plate.id, experiment.date, rowi, coli, wllq, rval, wllq_notes)] # all are in row 6, col 1
# dat4[AB.trt.finalized == FALSE & grepl("AB",acnm) & treatment == "Media", `:=`(treatment = "Lysis",conc = 10, AB.trt.finalized = TRUE)]

# # view updated stripchart
# plotdat <- dat4[treatment %in% c("DMSO","PICRO","TTX","BIC","Media","Lysis","? Lysis","1:250 LDH","1:2500 LDH") & grepl("(AB)",acnm)]
# view_activity_stripchart(plotdat, title_additions = "Media renamed to Lysis")

# for LDH assay:
plotdat <- dat4[treatment %in% c("DMSO","PICRO","TTX","BIC","Media","Lysis","? Lysis","1:250 LDH","1:2500 LDH") & grepl("(LDH)",acnm)]
view_activity_stripchart(plotdat, title_additions = "No Changes to Treatment Labels")

# looks like media wells really do just contain Media
# actually makes some sense based on the assay - 
# first, 50uL of culture Media from each well in MEA plate is transfered to LDH plate (so F1 just contains Media in LDH plate)
# then 20uL of Media is added to F1 in the MEA plate. Then (all?) of the contents are removed and mixed
# a Media/Alamar Blue mixture is added to eadch well of MEA plate
# then culture Media/Blue mixutre is taken from MEA plates to AB plates
# all this to say, I understand now why the 
# The only time when well F1 would have Lysis in the LDH assay is the same time when Lysis is added before the second recording,
# which was already clearly marked in the Group 1 file. 
# So LDH well treatments should usually = neural stats well treatments, if Lysis, PICRO, etc. were added before the second recording

# final treatment updates:
cat("Confirm that the rest of these treatments look normal (nothing NA, 0, etc):\n")
cat(dat4[, unique(treatment)], sep = ", ")
cat("\n")


# ASSIGN SPIDS
cat("\nAssign spid's:\n")
cat("Using spidmap file:",spidmap_file,"\n")
spidmap <- as.data.table(read_excel(spidmap_file, sheet = use_sheet))
names(spidmap)
setnames(spidmap, old = "NCCT ID", new = "spid")
setnames(spidmap, old = "Chemical ID", new = "treatment")
setdiff(unique(dat4$treatment), unique(spidmap$treatment))
# [1]   
dat4 <- merge(x = dat4, y = spidmap[, c("spid", "treatment")], all.x = TRUE, by = "treatment")

# assign spids for the non-registered control compounds, e.g.: "Tritonx100" "Bicuculline"  "DMSO" "PICRO" "TTX" "MEDIA"
dat4[is.na(spid),unique(treatment)]
# [1] 
dat4[grepl("DMSO",treatment), spid := "DMSO"]
dat4[treatment == "Media", spid := "Media"]
dat4[treatment == "PICRO", spid := "Picrotoxin"]
dat4[treatment == "TTX", spid := "Tetrodotoxin"]
dat4[grepl("Lysis",treatment), spid := "Tritonx100"]
dat4[grepl("Lysis",treatment), unique(conc), by = "treatment"]
unique(dat4$spid) # confirm no NA spids
if(any(is.na(unique(dat4$spid)))) {
  stop(paste0("The following treatments don't have a corresponding spid:", dat4[is.na(spid), unique(treatment)]))
} else {
  cat("No spids are NA.\n")
}
cat("Number of unique spids:",dat4[,length(unique(spid))],"\n")


# PREPARE LDH P WELLS (must verify wllq, treatments first)
dat4 <- prepare_LDH_p_wells(dat4)


# ASSIGN WLLT
dat4 <- assign_wllt(dat4)


# CHECK CONC'S
cat("\nFinalize Concentrations:\n")
dat4[, conc_original := conc]
dat4[, unique(conc)] # any NA's? any non-numeric? Any 0? does it look like conc correction was done?

# update conc for DMSO, PICRO, TTX, BIC, and full Lysis wells
# dmso
dat4[treatment == "DMSO",unique(conc)]
# [1] "Control"
# Use the percent DMSO by volume?
# dat4[treatment == "DMSO", conc := "0.001"]

# picro
dat4[treatment == "PICRO", .N, by = "conc"]
# 
# based on lab notebook, this is usually 25
# dat4[treatment == "PICRO", conc := "25"]

# ttx
dat4[treatment == "TTX", .N, by = "conc"]
# 
# based on lab notebook, this is usually 1
# dat4[treatment == "TTX", conc := "1"]

cat("\nConcentration Corrections:\n")
# any other compounds to update??
# need to do concentration correction??
cat("CHANGES MADE/rationale")
# cat("The following treatment have char conc. Will be set to NA:\n")
# print(suppressWarnings(dat4[is.na(as.numeric(conc)), .N, by = c("spid","treatment","conc")]))
# dat4[, conc := suppressWarnings(as.numeric(conc))]

# final updates, view conc's, make table of control conc's
dat4 <- assign_common_conc(dat4)


# ASSIGN ACID
cat("\nAssign ACId:\n")
cat("(not doing this for now, since new acnm's need to be registered)\n")
# dat4 <- add_acid(dat4) # holding off, need to register new acid's


# check that all data is there, nothing is missing, view plots
data_checks(dat4)

# closing graphics after last plots
graphics.off()

# create a nice summary of wllq assignments for each well
createWllqSummary(dat4, dataset_title)
cat("(note that the wllq is not quite final -\nwllq will be updated for outlier DMSO wells will before creating lvl 0 snapshot)\n")

# save dat4
dat4 <- dat4[, .(treatment, spid, experiment.date, plate.id, apid, rowi, coli, conc, acnm, wllt, wllq, wllq_notes, rval, srcf, dat3)]
save(dat4, file = file.path(main.output.dir, paste0("output/",dataset_title,"_dat4_",as.character.Date(Sys.Date()),".RData")))
cat("\ndat4 saved on:",as.character.Date(Sys.Date()), "\n")

# you're done!

```

# Merge in wllq tables

```{r}
wllq.tb.by.well <- as.data.table(read.csv(file.path(dataset_title, 'wells_quality_assignments_by_well.csv')))
wllq.tb.by.well.ldh <- wllq.tb.by.well[grepl('LDH',affected_endpoints)]
wllq.tb.by.well.ldh[, acsn := 'LDH']
wllq.tb.by.well.AB <- wllq.tb.by.well[grepl('(AB)|(CTB)',affected_endpoints)]
wllq.tb.by.well.AB[, acsn := 'AB']
wllq.tb.by.well.cyto <- rbind(wllq.tb.by.well.ldh, wllq.tb.by.well.AB)
```

# Scrap

## To fix the date in teh experiment start time
```{r}
# If I wanted to fix the date --------------------------------------------------

# - fix month (see testing below)
# - is the day every single-digit?
# - any risk the year is ever single-digit?
# (maybe just say)
dat1[grepl('[0-9]////[0-9]{2}////[0-9]{4}',experiment_start_time), est_date_format := '%m/%d/%Y']
dat1[grepl('[0-9]{2}////[0-9]{2}////[0-9]{4}',experiment_start_time), est_date_format := '%M/%d/%Y']
dat1[is.na(est_date_format)]
# BUUUUT, maybe it's okay if the date is wrong? -> yeah, let's go with that
# so let's just grab the time and sort that way

# testing -------------------------------------------------------------------------
as.POSIXlt(x = '11:37:00', format = '%H:%M:%OS')
as.numeric(as.POSIXlt(x = '11:37:00', format = '%H:%M:%OS'))
as.POSIXlt.character(x = '11:37:00', format = '%H:%M:%OS')

# note that the "m" must match the format of the month
as.POSIXlt('8/24/2021 1:34:26 PM', format = '%M/%d/%Y %H:%M:%OS %p')
# [1] "2021-05-24 01:34:26 EDT"
# month is 05, instead of 08
as.POSIXlt('8/24/2021 1:34:26 PM', format = '%m/%d/%Y %H:%M:%OS %p')
# "2021-08-24 01:34:26 EDT"
# correct!
```

## Determine run type

Old code:

```{r}
# RESUME HERE --------------------------------
# Motivation: run type tags in these files are quite inconsistent
# Would be so much easier to just use the file times as reported by axis
# dat1 from 12/30/2021 was made with updated level 1 functions
# contains exp start time, original file time,
# but no wllq or run type

# Any values NA?
dat1 <- get_latest_dat('dat1', dataset_title)
dat1[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(dat1)]
# So the activity_value is the only column that is NA sometimes -> didn't actually have to use fill = T

# Resolving warnings of missing plate.id's
dat1[plate.id == 'MW', .N, by = .(experiment.date, srcf)]
#    experiment.date                                                              srcf    N
# 1:        20210824 AC_20210811_MW75-9119_13(000)_Neural Statistics Compiler(000).csv 2064
# 2:        20210824 AC_20210811_MW75-9119_13(001)_Neural Statistics Compiler(000).csv 2064
# 3:        20210824 AC_20210811_MW75-9120_13(000)_Neural Statistics Compiler(000).csv 2064
# 4:        20210824 AC_20210811_MW75-9120_13(001)_Neural Statistics Compiler(000).csv 2064
# 5:        20210824 AC_20210811_MW75-9201_13(000)_Neural Statistics Compiler(000).csv 2064
# 6:        20210824 AC_20210811_MW75-9201_13(001)_Neural Statistics Compiler(000).csv 2064
dat1[plate.id == 'MW', plate.id := stri_extract_first(srcf, regex = 'MW[0-9\\-]{7}')]
dat1[experiment.date == '20210824', .N, by = .(srcf, plate.id)]
#                                                                 srcf  plate.id    N
# 1: AC_20210811_MW75-9119_13(000)_Neural Statistics Compiler(000).csv MW75-9119 2064
# 2: AC_20210811_MW75-9119_13(001)_Neural Statistics Compiler(000).csv MW75-9119 2064
# 3: AC_20210811_MW75-9120_13(000)_Neural Statistics Compiler(000).csv MW75-9120 2064
# 4: AC_20210811_MW75-9120_13(001)_Neural Statistics Compiler(000).csv MW75-9120 2064
# 5: AC_20210811_MW75-9201_13(000)_Neural Statistics Compiler(000).csv MW75-9201 2064
# 6: AC_20210811_MW75-9201_13(001)_Neural Statistics Compiler(000).csv MW75-9201 2064
# looks good!!

# Next step is to assign the run_type
# (see determine_run_type -> next need to address some formatting in times, then how to check all 5 run type assignment methods),
# followed by assigning the wllq
# Then make this flow into existing level 2

# Determine the run type based on ordering of the run_type tag
# (which was determined by the first tag in the file names that 
# is unique for each pair of consecutive files when sort by file name) 
file.names.split <- stri_split(str = names(run.type.tag.location), fixed = '_')
run.type.tags <- unlist(lapply(1:length(run.type.tag.location), function(i) file.names.split[i][[1]][run.type.tag.location[i]]))
run.type.tag.location.tb <- data.table('srcf' = names(run.type.tag.location), 
                                       'run.type.tag.location' = run.type.tag.location,
                                       'run.type.tag' = run.type.tags)
dat1 <- merge(dat1, run.type.tag.location.tb, by = 'srcf', all.x = T)
dat1[, file_run_type_tag_rank := frank(run.type.tag, ties.method = 'dense'), by = .(experiment.date, plate.id)]

# Convert file times from character to a comparable numeric value, e.g. POSIX?
# e.g. as.POSIXct(..., format = ...)
dat1[, experiment_start_time_posix := as.POSIXlt(experiment_start_time, format = c('%M/%d/%Y %H:%M:%OS')), by = .(srcf)]
dat1[, original_file_time_posix := as.POSIXlt(original_file_time, format = c('%M/%d/%Y %H:%M:%OS')), by = .(srcf)]

# Check for and fix any NAs in POSIX file time (time may be in incorrect format, or AM/PM did not copy from csv)
dat1[is.na(experiment_start_time_posix) | is.na(original_file_time_posix), .N, by = .(srcf, original_file_time, experiment_start_time)]
#                                         srcf original_file_time experiment_start_time    N
# 1: AC_20210428_MW75-8205_15_00(000)(000).csv    5/13/2021 12:56       5/13/2021 12:33 2112
# How does the treated file look?
dat1[grepl('AC_20210428_MW75-8205_15',srcf), .N, by = .(srcf, original_file_time)]
#                                         srcf  original_file_time    N
# 1: AC_20210428_MW75-8205_15_00(000)(000).csv     5/13/2021 12:56 2112
# 2: AC_20210428_MW75-8205_15_00(001)(000).csv 05/13/2021 14:03:00 2112
# huh, so the dates in the treated file are formatted normally
# I confirmed in the csv file that 5/13/2021 12:56 is "PM" ;)
dat1[srcf == 'AC_20210428_MW75-8205_15_00(000)(000).csv', 
     original_file_time_posix := as.POSIXlt('05/13/2021 12:56:00', format = c('%M/%d/%Y %H:%M:%OS'))]
dat1[srcf == 'AC_20210428_MW75-8205_15_00(000)(000).csv', 
     experiment_start_time_posix := as.POSIXlt('05/13/2021 12:33:00', format = c('%M/%d/%Y %H:%M:%OS'))]

# code snip for Kelly
dat1[srcf == 'AC_20210428_MW75-8205_15_00(000)(000).csv', .N] # 2112
as.POSIXlt('05/13/2021 12:56:00', format = c('%M/%d/%Y %H:%M:%OS'))
dat1[srcf == 'AC_20210428_MW75-8205_15_00(000)(000).csv', original_file_time_posix := as.POSIXlt('05/13/2021 12:56:00', format = c('%M/%d/%Y %H:%M:%OS'))]
dat1[srcf == 'AC_20210428_MW75-8205_15_00(000)(000).csv', .N, by = .(original_file_time_posix)]
class(as.POSIXlt('05/13/2021 12:56:00', format = c('%M/%d/%Y %H:%M:%OS')))
class(dat1$original_file_time_posix)

dat1[srcf == 'AC_20210428_MW75-8205_15_00(000)(000).csv', 
     original_file_time_posix := as.Date(as.POSIXlt('05/13/2021 12:56:00', format = c('%M/%d/%Y %H:%M:%OS')))]

dat1[, original_file_time_posix := as.Date(as.POSIXlt('05/13/2021 12:56:00', format = c('%M/%d/%Y %H:%M:%OS')))]
dat1[, original_file_time_posix2 := as.POSIXlt('05/13/2021 12:56:00', format = c('%M/%d/%Y %H:%M:%OS'))]


# Determine the run type based on 3 other methods -> these methods are likely more reliable, but will compare them all
dat1[, file_exp_start_time_rank := frank(experiment_start_time_posix, ties.method = 'dense'), by = .(experiment.date, plate.id)]
dat1[, file_original_file_time_rank := frank(original_file_time_posix, ties.method = 'dense'), by = .(experiment.date, plate.id)]
dat1[, file_name_rank := frank(srcf, ties.method = 'dense'), by = .(experiment.date, plate.id)]

# How to compare all columns?
# one idea...
dat1[, multiple_unique_ranks := 
       pmax(file_run_type_tag_rank, file_exp_start_time_rank, file_original_file_time_rank, file_name_rank)
     - pmin(file_run_type_tag_rank, file_exp_start_time_rank, file_original_file_time_rank, file_name_rank)]




=======
  >>>>>>> master
# OUTPUT --------------------------------------------------------- 
# Level 1 - Extract All Data:
#   
#   Reading from TSCA2019_neural_stats_files_log_2021-12-16.txt...
# Got 220 files.
# Reading data from files...
# Processed AC_20201125_MW71-7111_13_00(001)(000).csv 
# Processed AC_20201125_MW71-7111_13_00(002)(000).csv 
# Processed AC_20201125_MW71-7112_13_00(000)(000).csv 
# Processed AC_20201125_MW71-7112_13_00(001)(000).csv 
# Processed AC_20201125_MW71-7115_15_00(000)(000).csv 
# Processed AC_20201125_MW71-7115_15_00(001)(000).csv 
# ...
# TSCA2019_dat1_2021-12-16.RData is ready.
# Summary of dates/plates with wllq=0 at Level 1:
# (73 plates afffected)
# over 50 instances of this warning:
# Warning messages:
# 1: In fileToLongdat(new_files[i], run.type.tag.location[i],  ... : 
#                       run type cannot be determined for AC_20210505_MW75-8207_13_35(000)(000).csv.
#                     No wllq checks will be done for this recording.
# ---------------------------------------------------------------- 

# Options:
# - try to determine how the function works to determine the run type, edi tit so that all my file names will be interpretted correctly
# - just assign the run_type here, and re-do the wllq assignments here

# Let's just review how the function works,
# then see how bad this situation is (can I write a simple rule, or much easier to just fix on a case by case basis here?)



# view dat1
dat1 <- get_latest_dat(lvl = "dat1", dataset_title)

dat1[, .N, by = .(run_type)]
#        run_type      N
# 1:     baseline 301344
# 2: 35(000)(000)  40128
# 3: 35(001)(000)  38016
# 4: 35(002)(000)  10560
# 5: 35(003)(000)   8448
# 6: 35(004)(000)   8448
# 7: 35(005)(000)   8448
# 8: 35(006)(000)   4224
# 9: 35(007)(000)   4224
# 10: 35(008)(000)   4224
# 11: 35(009)(000)   4224
# 12: 35(010)(000)   4224
# 13: 35(011)(000)   4224
# 14: 15(000)(000)   4224
# 15: 15(001)(000)   4224

# Hmm... I'm curious if all/most of those not labelled baseline are just treated,
# perhaps not much to sort through?
dat1[, .N, by = .(run_type == 'baseline' | run_type == '35(000)(000)')]
# run_type      N
# 1:     TRUE 341472
# 2:    FALSE 107712
# Nope, there are far more that are currently labelled "baseline",
# So I'm guessing that several that are labelled baseline
# are not actually baseline

dat1[run_type == 'baseline', .N, by = .(srcf)]
#                                         srcf    N
# 1: AC_20201104_MW71-7104_13_00(000)(000).csv 2064
# 2: AC_20201104_MW71-7104_13_00(001)(000).csv 2064
# 3: AC_20201104_MW71-7105_13_00(001)(000).csv 2064
# 4: AC_20201104_MW71-7105_13_00(002)(000).csv 2064
# Oh yeah, this is definitely not right


# RESUME HERE -------------------------------------------------------------

# Try to figure out how to assign the run type,
# first just in this code, then see if you can translate a rule for most cases to fileToLongdat()
# If you need to ask Kathleen or ask her to rename in some cases, that's valid too
# But also thinking about the future... we need something that's goign to be dummy-proof
# (either a hard and fast naming rule, or )


# other things could check (from running this a logn time ago:) ------------
print(dat1[, .N/length(unique(dat1$acnm)), by = "wllq_notes"])
# view all experiment.date's and plate.id's. Are there any NA/missing labels?

# making sure the baseline/treatment labelling looks correct
dat1[, .N, by = .(apid, srcf, run_type)] # oh dear, all are labelled baseline rn!

# add teh run type tag location to each srcf
run.type.tag.tb <- data.table(srcf = names(run.type.tag.location), run.type.tag.location = run.type.tag.location)
dat1 <- merge(dat1, run.type.tag.tb, by = 'srcf')
dat1[run.type.tag.location == 5, run.type.tag := stri_replace_all_regex(srcf, pattern = paste0(c(rep('[^_]*_',times=4)),collapse=''), replacement = '')]
dat1[run.type.tag.location == 6, run.type.tag := stri_replace_all_regex(srcf, pattern = paste0(c(rep('[^_]*_',times=5)),collapse=''), replacement = '')]
dat1[, run.type.tag := stri_replace_all_regex(run.type.tag, pattern = '\\.csv', '')]
dat1[, baseline.run.type.tag := sort(unique(run.type.tag))[1], by = .(apid, plate.id, experiment.date)]
dat1[, run_type := ifelse(run.type.tag == baseline.run.type.tag, 'baseline', 'treated')]
dat1[, .N, by = .(apid, srcf, run_type, run.type.tag)] # looks good!

# At Kathleen's request, getting the recording name for each file
extract_recording_name <-  function(filei) {
  
  file_scan <- scan(file = filei, what = character(), sep = "\n", blank.lines.skip = F, quiet=T) # empty lines will be just ""
  file_col1 <- sapply(file_scan, function(x) strsplit(x, split = ",")[[1]][1], USE.NAMES = F) # empty lines will be NA
  file_col2 <- sapply(file_scan, function(x) strsplit(x, split = ",")[[1]][2], USE.NAMES = F) # if nothing in second col, will be NA
  
  # get relevant data from file header
  headdat <- data.table(file_col1, file_col2)
  
  rec.name <- headdat[grepl('Recording Name',file_col1), file_col1]
  return(data.table(recording_name = rec.name, srcf = basename(filei)))
  
}

files <- read_files(main.output.dir)
# Reading from TSCA2019_neural_stats_files_log_2021-05-10.txt...
# Got 84 files.
setdiff(basename(files), unique(dat1$srcf)) # empty!
add.dat <- data.table()
for (filei in files) {
  add.dat <- rbind(add.dat, extract_recording_name(filei))
}
str(add.dat)
dat1 <- merge(dat1, add.dat, by = 'srcf', all = T)
dat1[is.na(recording_name)] # empty
dat1[is.na(run_type)] # empty
rm(add.dat)

# export the requested data to excel
# I'm going to try to note the group, from the srcf name
filename.tb <- data.table(srcf = basename(files), fullname = files)
dat1 <- merge(dat1, filename.tb, by = 'srcf', all = T)
dat1[, .N, by = .(fullname)]
dat1[, culture_folder := basename(dirname(dirname(fullname)))]
dat1[, .N, by = .(culture_folder)]

# I'm pretty sure they won't want the txt prefix in the recording name
dat1[, recording_name := sub('Recording Name: ','',recording_name)]
dat1[, culture.date := sub(' .*$','',culture_folder)]
dat1[culture.date != recording_name, .N, by = .(culture.date, recording_name)]
# not all equa
dat1[experiment.date != recording_name, .N, by = .(culture.date, recording_name)] # many cases
# I guess I'll include all 3 for now
dat1[, group := sub('^[^G]* ','',culture_folder)]
dat1[, .N, by = .(group)]

prep.dat <- dat1[run_type == 'baseline' & grepl('^2021',experiment.date) & acsn %in% c('Weighted Mean Firing Rate (Hz)','Number of Active Electrodes'), 
                 .(culture.date, group, experiment.date, recording_name, plate.id, well, acsn, activity_value, wllq, wllq_notes)]
setnames(prep.dat, old = c('experiment.date','plate.id'), new = c('experiment_date','plate'))
prep.dat2 <- dcast(prep.dat, culture.date + group + experiment_date + recording_name + plate + well + wllq + wllq_notes ~ acsn, value.var = 'activity_value')
prep.dat2

prep.dat2[, .N, by = .(culture.date, group, experiment_date)]

wb <- createWorkbook()
openxlsx::addWorksheet(wb, sheetName = 'Sheet1')
writeData(wb, sheet = 1, x = prep.dat2)
saveWorkbook(wb, file = 'TSCA2019_MEA_Acute_baseline_recordings_from_2021.xlsx', overwrite = T)
rm(dat1)
```

## Code snippets cut from fileToLongdat

### determining run type

```{r}
# determine run type from filei
# _00 is for baseline. _01 is for treated
# user enters run.type.tag.location in input file names (usually 5)
# e.g. TC_20190508_MW68-0808_13_00(000).csv is baseline, and TC_20190508_MW68-0808_13_01(000).csv is treated.
# ignore the 0's and 1's that come after the first 2 digits in that tag
run.type.tag <- strsplit(basename(filei), split = "_")[[1]][run.type.tag.location]
if (guess_run_type_later) {
  run_type <- sub("\\.csv","",run.type.tag)
} else{
  run_type <- switch(substring(run.type.tag,1,2), 
                     "00" = "baseline",
                     "01" = "treated",
                     sub("\\.csv","",run.type.tag))
}
if(!run_type %in% c('baseline','treated')) warning(paste0("\nrun type cannot be determined for ",basename(filei),'.\nNo wllq checks will be done for this recording.'))

```


# Overall checks

## Check that all parameters are registered in invitrodb


**Replace below with just checking that all export_ready == 1 endpoitns are present


Note that different maestros (and possibly different axis versions) will output slightly different subsets of endpoints. So, just want to check that there are no unexpected new endpoints.

```{r}
setdiff(dat1$acsn, acsn_map[Status_2023.05.12 == 'registered in invitrodb',unique(acsn)])
```

"Network ISI Coefficient of Variation" - this is not an endpoint that has export_ready == 1, so this is okay (even though it is registered).

```{r}
# Note total parameters
dat1[, param_total := length(unique(acsn)), by = .(neural_stats_file)]
dat1[, registered_param := length(unique(acsn[acsn %in% acsn_map[Status_2023.05.12 == 'registered in invitrodb',acsn]])), by = .(neural_stats_file)]
dat1[, not_registered_param := length(unique(acsn[acsn %in% acsn_map[Status_2023.05.12 != 'registered in invitrodb',acsn]])), by = .(neural_stats_file)]
dat1[, .(num_files = length(unique(neural_stats_file))),
     by = .(param_total, registered_param, not_registered_param)]

# Note which of the registered parameters are missing (will vary by maestro type)
registered.params <- acsn_map[Status_2023.05.12 == 'registered in invitrodb',unique(acsn)]
res <- dat1[, .(missing_params = paste0(setdiff(registered.params, acsn),collapse = ",")), by = .(neural_stats_file, setting_axis.version)]
res[, .N, by = .(missing_params, setting_axis.version)]

```

I am confident that no one is haphazardly selected which parameters to include. I think it all depends on the versions of the machine/software. So we'll just include all provided info and keep rolling.

Confirm that the main 15 are present in every file

```{r}
library(tcpl)
tcplConf(drvr = 'MySQL')
dat1[]
```

