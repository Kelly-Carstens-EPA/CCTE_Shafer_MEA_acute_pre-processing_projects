---
title: 'Pre-process TSCA2019 MEA Acute data'
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
    df_print: paged
date: "May 11, 2023"
---

# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries

```{r}
rm(list = ls())
library(data.table)
library(openxlsx)
library(stringi)
library(ggplot2)

print(sessionInfo())
```


## Project variables definitions

```{r}
project_name <- "TSCA2019" # e.g. "name2020"

spidmap_file <- "L:/Lab/NHEERL_MEA/Project TSCA 2019/EPA_25092_EPA-Shafer_339_20190722.xlsx"
spid_sheet <- 1 # sheet name in spidmap_file

project.input.dir <- "L:/Lab/NHEERL_MEA/Project TSCA 2019/Acute TSCA Conc Response"
scripts.dir <- "mea-acute-neural-stats-to-mc0-scripts"
root.output.dir <- ""
assay_component_map_filename <- file.path("neural_stats_acsn_to_tcpl_acnm_map.xlsx")

select.neural.stats.files <- F # select new neural stats files, or use the files in the most recent neural_stats_files_log?
select.calculations.files <- F # select new calculations files, or use the files in the most recent calculations_files_log?
select.raw.cytotox.files <- F # select new raw cytotoxicity files, or use the files in the most recent raw_cytotox_files_log?
run.type.tag.location <- NULL # neural stats files should be named as "tag1_tag2_tag3_....csv". Which tag in the file names defines the run type?

# optional adjutsment; usually can use defaults:
override_wllq_checks <- FALSE # set to TRUE only if you have already verified your wllq updates
plate.id.tag.location <- numeric(0) # only update this if you have to, if your dataset does not include plate.id.tag in file headers
noisy_functions <- FALSE
standard_analysis_duration_requirement <- TRUE # default should be true (recordings that are shorten than this length will be set to wllq == 0)
```


Set up folders, source functions, and load ascn-acnm map
```{r}
if (!dir.exists(file.path(project_name))) dir.create(file.path(project_name))
if (!dir.exists(file.path(project_name,"output"))) dir.create(file.path(project_name,"output"))

# source all functions in folder 'mea-acute-neural-stats-to-mc0-scripts'
scripts <- list.files(path = scripts.dir, pattern = "\\.R$", full.names = T, recursive = F)
for (scripti in scripts)
  source(scripti)

# loading acsn_acnm map
acsn_map <- as.data.table(read.xlsx(assay_component_map_filename))
acsn_map <- acsn_map[, .(acsn, acnm)]
rm(assay_component_map_filename)
```


Save items to be kept throughout all levels
```{r}
keep.items <- c(ls(),'keep.items')
```


# Level 0 - Gather and Check Files

## Read any .txt files with important notes

Scan for txt files with notes that might affect dosing, wllq
```{r}
txt.files <- list.files(project.input.dir, pattern = '\\.txt', recursive = T, full.names = T)
```

View any README .txt files
```{r eval = F}
# Read in the READMEs
readmes.filename <- txt.files[grepl('read( )*me',tolower(txt.files))]
readmes.body <- sapply(readmes.filename, scan, what = character(), sep = '\n', quiet = T) # read the text files
readmes.body <- sapply(readmes.body, paste0, collapse = "\n") # Make each note 1 string

# View the readmes that have any non-generic text
if (length(readmes.body) > 0) {
  for (i in 1:length(readmes.body)) {
    cat(i,'\n')
    cat(names(readmes.body)[i],'\n')
    cat(readmes.body[i])
    cat('\n\n')
  }
}
# 1 
# L:/Lab/NHEERL_MEA/Project TSCA 2019/Acute TSCA Conc Response/20210526 Culture G26 (201B07, 201C02, 201C03 may need repeat) 
# Plate 1 CellTiter Blue blank control is not valid
# blank values are taken from plate 2 blank controls
```

5/11/23: Seline has already updated the formula in the Calculations file such that the corrected optical density values for plate 1 are corrected to the blanks from plate 2. No further updates needed.

Check for any other .txt files that look relevant
```{r eval = F}
cat('other txt files:\n')
cat(setdiff(txt.files,readmes), sep = "\n")
# (if there are any other txt files that look like they might have relevant notes, review the contents of those files as well)
```

Update logs are just files I made to note updates to the Calculations files - these are fine.

## Get list culture folders to review

Get a list of all cultures in the main folder to check off as I review wllq notes in lab notebook and determine usability.

`eval = FALSE` because I only need to run this once

```{r eval = FALSE}
# Just want to get a list of all cultures in the main folder
group.folders <- list.files(path = project.input.dir, pattern = '[0-9]{8}', include.dirs = T)
cat(group.folders, sep ='\n')
wb <- createWorkbook()
addWorksheet(wb, 'TSCA cultures')
writeData(wb, 1, data.table('culture_folders' = group.folders))
saveWorkbook(wb, file = 'TSCA2019/tables/TSCA2019_culture_folders_to_review.xlsx')
```

## Get list of neural stats files, meta data files, and (opt) raw cytotoxicity data files

```{r eval = F}
# For each culture, get all files under "Neural Statistic Compiler" folder
# if there are not exactly 6 files per folder -> flag (including if there are just 0)
group.folders <- list.files(path = project.input.dir, 
                            pattern = '[0-9]{8}',
                            include.dirs = T,
                            full.names = T)
print(basename(group.folders))
# (edit the list of group folders to include if necessary)
```

Looks okay!

Select input files to use, store files in .txt file
```{r eval = F}
if (select.neural.stats.files) {
  
  # Use below lines of code to get all files in folder "Neural Statistic Compiler" for each group
  # Or, if project is not cleanly organized, use below function to cycle through 
  # folders in project and manually select files using the 'choose.files' window interface
  # selectInputFiles(project.input.dir, project_name, files_type = "neural_stats")
  
  # Get all neural stats files in folder
  neural.stats.files <- sapply(group.folders, 
                               function(folderi) list.files(path = file.path(folderi,
                                                                             'Neural Statistic Compiler'), 
                                                            pattern = '\\.csv', 
                                                            full.names = T,
                                                            recursive = T))
  
  # Check that there are exactly 6 neural stats files per group
  # (1 baseline and 1 treated X 3 plates)
  num.files.per.group <- lapply(neural.stats.files, length)
  cat('Groups that did not match exactly 6 neural stats files:')
  num.files.per.group[num.files.per.group != 6]
  
  # (edit the list of neural.stats.files if needed)
  
  # Exclude 20201127_MW71-7113 (because treated recording broken up), for now (will investigate Seline's methods to combine and if I agree if time later)
  # Similar situation for G24, plate 75-8213 
  neural.stats.files <- sapply(neural.stats.files,
                               function(files) files[!grepl('20201125_MW71-7113',files)])
  neural.stats.files <- sapply(neural.stats.files,
                               function(files) files[!grepl('20210512_MW75-8213',files)])
  num.files.per.group <- lapply(neural.stats.files, length)
  
  # Final check:
  # stopifnot(sum(num.files.per.group != 6) == 0)
  # (removing this check because there are 2 groups with 1 plate removed)
  # (so I'll just confirm that there are an even # of files per group)
  stopifnot(sum(unlist(num.files.per.group) %% 2) == 0)
  
  # Write files to log
  writeLogFile(unlist(neural.stats.files), project_name, files_type = 'neural_stats')
  
}
```

Select calculations files
```{r eval = F}
if (select.calculations.files) {
  
  # Use below lines of code to get files in group folders that contain the phrase "Calculations"
  # Or, if project is not cleanly organized, use below function to cycle through 
  # folders in project and manually select files using the 'choose.files' window interface
  # selectInputFiles(project.input.dir, project_name, files_type = "calculations")
  
  # Get all calculations files in folder
  calc.files <- sapply(group.folders, 
                       function(folderi) list.files(path = file.path(folderi), 
                                                    pattern = 'Calculations', 
                                                    full.names = T,
                                                    recursive = F))
  
  # Remove any dummy "ghost" files
  calc.files <- sapply(calc.files,
                       function(files) files[!grepl('\\~\\$',basename(files))])
  
  # Check that there is exactly 1 calc file per group
  num.files.per.group <- lapply(calc.files, length)
  cat('Groups that did not match exactly 1 Calculations file:')
  num.files.per.group[num.files.per.group != 1]
  
  # (edit the list of calc.files if needed)
  
  # Final check:
  stopifnot(sum(num.files.per.group != 1) == 0)
  
  # Write files to log
  writeLogFile(unlist(calc.files), project_name, files_type = 'calculations')
  
}
```

Select raw cytotoxicity data files

```{r eval = F}
if (select.raw.cytotox.files) {
  
  # Use below lines of code to get files in group folders that contain the phrase "Calculations"
  # Or, if project is not cleanly organized, use below function to cycle through 
  # folders in project and manually select files using the 'choose.files' window interface
  # selectInputFiles(project.input.dir, project_name, files_type = "calculations")
  
  # Get all cytotox files
  # (note that namesa re not retained with recursive = T)
  all.xls.files <- unlist(lapply(group.folders, 
                                 function(folderi) list.files(path = file.path(folderi), 
                                                              pattern = '\\.xls$', 
                                                              full.names = T, 
                                                              recursive = T)))
  raw.cytotox.files.ungrouped <- all.xls.files[grepl('Cytotoxicity',all.xls.files)]
  all.group.folders <- dirname(dirname(raw.cytotox.files.ungrouped))
  raw.cytotox.files <- split(raw.cytotox.files.ungrouped, f = all.group.folders)
  
  # Check that there is exactly 1 LDH file per group
  num.LDH.per.group <- sapply(raw.cytotox.files, function(seti) sum(grepl('LDH',seti)))
  cat('Groups that do not have exactly 1 LDH file:')
  num.LDH.per.group[num.LDH.per.group != 1]
  
  # (if any, edit selection or rename files if missing "LDH" in name)
  
  # Check that there are exactly 3 AB files per group
  num.AB.per.group <- sapply(raw.cytotox.files, function(seti) sum(!grepl('LDH',unlist(seti))))
  cat('Groups that do not have exactly 3 AB file:')
  num.AB.per.group[num.AB.per.group != 3]
  
  # Write files to log
  writeLogFile(unlist(raw.cytotox.files), project_name, files_type = 'raw_cytotox')
  
}
```


# Level 1 

## Read neural stats files

Extract all of the data from the files and transform into long data format (dat1)

```{r eval = F}
dat1 <- extractAllData(project_name, 
                       acsn_map,
                       append = F, 
                       plate.id.tag.location = NULL,
                       noisy_functions = noisy_functions)
```

Get plate.id from filename for files that don't have plate ID in file header (because were ran with older machine/software)

```{r eval = F}
dat1[is.na(plate.id) | plate.id %in% c('MW'), .N, by = .(neural_stats_file, plate.id)]
dat1[, check_plate_ids := is.na(plate.id) | plate.id %in% c('MW')]
dat1[, .N, by =.(setting_axis.version, check_plate_ids)] # all the plates that dont' have plate ids come from the same axis version
dat1[is.na(plate.id) | plate.id %in% c('MW'), plate.id := stri_extract(neural_stats_file, regex = 'MW[^_]+')] # note that sometimes the 
dat1[check_plate_ids == TRUE, .N, by = .(plate.id, neural_stats_file)]
```

Looks good!

## Determine run type for each file

Get a numeric, sortable version of the time from the original_start_time and experiment start time (to use to help determine which files are baseline versus treated).

```{r eval = F}
dat1 <- getNumericTimeValFromString(dat1, time_string = 'original_file_time', new_cols_suffix = 'oft')
dat1 <- getNumericTimeValFromString(dat1, time_string = 'experiment_start_time', new_cols_suffix = 'est')
```

Rank the files associated with each experiment.date - plate.id combination based on the experiment start time, original file time, and neural_stats_file (name). Ideally, each experiment.date - plate.id combination will correspond to 2 files, with 1 file clearly corresponding to the chronologically first "baseline" recording and the other the secondary "treated" recording. Ideally this ranking would be agree for all 3 methods.

```{r eval = F}
# Determine the run type based on the 2 times given in file header
dat1[, file_rank_oft := frank(time_posix_num_oft, ties.method = 'dense'), 
     by = .(experiment.date, plate.id)]
dat1[, file_rank_est := frank(time_posix_num_est, ties.method = 'dense'), 
     by = .(experiment.date, plate.id)]

# Determine ranking based on file name
dat1[, file_rank_name := frank(neural_stats_file, ties.method = 'dense'), 
     by = .(experiment.date, plate.id)]
```

Check for discrepancies in file rank from the 3 methods
```{r eval = F}
dat1[, .N, by = .(file_rank_oft, file_rank_name, file_rank_est)]
#   file_rank_est file_rank_oft file_rank_name      N
# 1:             1             1              1 228672
# 2:             2             2              2 222480
# 3:             1             2              2   6192

if (nrow(dat1[!(file_rank_est == file_rank_oft & file_rank_oft == file_rank_name)]) != 0) {
  warning('Rankings of files for each experiment date and plate.id pair by experiment start time, original file time, and file name do not all agree')
}
```

Investigate discrepancies
```{r eval = F}
dat1[file_rank_est != file_rank_oft, .N, by = .(experiment.date, plate.id, experiment_start_time, original_file_time)]
#    experiment.date  plate.id experiment_start_time  original_file_time    N
# 1:        20201208 MW71-7111   12/08/2020 10:32:25 12/08/2020 12:04:05 2064
# 2:        20201208 MW71-7112   12/08/2020 12:45:12 12/08/2020 14:10:03 2064
# 3:        20210202 MW72-8209   02/02/2021 12:46:36 02/02/2021 14:18:38 2064
dat1[, date_plate := paste0(experiment.date, '_',plate.id)]
check.dps <- dat1[file_rank_est != file_rank_oft, unique(date_plate)]
dat1[date_plate %in% check.dps, .N, by = .(neural_stats_file, file_rank_est, file_rank_oft, experiment_start_time, original_file_time)]
#                            neural_stats_file file_rank_est file_rank_oft experiment_start_time  original_file_time    N
# 1: AC_20201125_MW71-7111_13_00(001)(000).csv             1             1   12/08/2020 10:32:25 12/08/2020 10:52:26 2064
# 2: AC_20201125_MW71-7111_13_00(002)(000).csv             1             2   12/08/2020 10:32:25 12/08/2020 12:04:05 2064
# 3: AC_20201125_MW71-7112_13_00(000)(000).csv             1             1   12/08/2020 12:45:12 12/08/2020 13:05:13 2064
# 4: AC_20201125_MW71-7112_13_00(001)(000).csv             1             2   12/08/2020 12:45:12 12/08/2020 14:10:03 2064
# 5: AC_20210120_MW72-8209_13_00(000)(000).csv             1             1   02/02/2021 12:46:36 02/02/2021 13:08:34 2064
# 6: AC_20210120_MW72-8209_13_00(001)(000).csv             1             2   02/02/2021 12:46:36 02/02/2021 14:18:38 2064
```
Okay, so we have 3 plates where the experiment start time is the same in both the treated and recording files. I could probably just take the original file time and use that, especially since it agrees with the file_rank_name.

Usually, the original file time is about 20 minutes after the experiment start time. 

```{r eval = F}
dat1[, .N, by = .(experiment.date, plate.id, experiment_start_time, original_file_time, file_rank_name)][order(experiment.date, plate.id)]
```

Most likely, the experiment start time corresponds to when the plate was put in the machine for the baseline recording or when the plate was dosed for the treated (so perhaps the experiment start time resets every time with lid is opened? Then the original file time would correspond to when the recording actually starts.

So it appears that for these 3 plates (MW71-7111, MW71-7112, and MW72-8209), the experiment start time did not reset after the dosing (perhaps these plates were ran on a different machine that didn't require opening the lid?)

```{r eval = F}
dat1[, .N, by = .(setting_axis.version, ranks_agree = file_rank_est == file_rank_oft)]
```

Well, there isn't 1 axis version that explains this consistently. 

Regardless, I think we can rely on the ranking of the original file time and the file names to identify the treated and baseline recordings for these 3 plates.

Make final run_type determinations
```{r eval = F}
# Determine the file type - here I'm going to use the consensus of the file_rank_oft and the file_rank_name
dat1[file_rank_oft == 1 & file_rank_name == 1, run_type := 'baseline']
dat1[file_rank_oft == 2 & file_rank_name == 2, run_type := 'treated']
stopifnot(nrow(dat1[is.na(run_type)]) == 0)
```

Confirm there is exactly 1 baseline and 1 treated file per experiment/plate

```{r eval = F}
dat1[, .(num_base_files = length(unique(neural_stats_file[run_type == 'baseline'])),
         num_trt_files = length(unique(neural_stats_file[run_type == 'treated']))),
     by = .(experiment.date, plate.id)][num_base_files != 1 | num_trt_files != 1]
```



## Setting wllq_lvl1 based on run type

```{r eval = F}
dat1 <- level1_set_wllq(dat1, standard_analysis_duration_requirement)
```

Summarize output of wllq_lvl1 (taken from extract all data)

```{r eval = F}
dat1[, .(num_wells= length(unique(paste0(experiment.date,plate.id,well)))), by = .(wllq_lvl1, run_type, wllq_lvl1_notes)][order(run_type, -wllq_lvl1)]
```

Note that all treated run_types are expected to have wllq_lvl1 == NA at this point, unless analysis duration was outside the allowable range (then would have wllq_lvl1 == 0).


## Check consistency in settings in Neural Statistics Compiler header

```{r eval = F}
files <- read_files(project_name)
settings.list <- lapply(files, getNeuralStatsHeaderDat)
set.tb <- rbindlist(settings.list)
rm(settings.list)

# Remove rows that are not really settings and are already captured in dat1
set.tb <- set.tb[!setting %in% c('Original File Time','Experiment Start Time','Plate Serial Number')]
```

Check if all files have all the same setting types (informs which maestro version used)

```{r eval = F}
# Check if all files have the same setting types
set.tb[, .(setting_types = paste0(unique(setting_type),collapse = ",")), by = .(neural_stats_file)][, .N, by = .(setting_types)]
#                                                                                                                setting_types   N
# 1: Maestro Pro Settings,Digital Filter Settings,Spike Detector Settings,Burst Detector Settings,Statistics Compiler Settings 212
# 2:     Maestro Settings,Digital Filter Settings,Spike Detector Settings,Burst Detector Settings,Statistics Compiler Settings   6
```
ah - 212 files have "Maestro Pro Settings", 6 files have "Maestro Settings". 
Probably because these 6 used a different Maestro was used (original maestro vs Maestro Pro)
```{r eval = F}
set.tb[, maestro_type := unique(sub(' Settings','',setting_type[grepl('Maestro',setting_type)])), by = .(neural_stats_file)]
set.tb[, .N, by = .(maestro_type)]
# My guess is that these 6 files have settings that are common for that maestro, but not the Maestro Pro
```

Confirm that all settings within a given pair of treated and baseline recordings is consistent

```{r eval = F}
# merge in run_type determination by file name
set.tb <- merge(set.tb, dat1[, unique(.SD), .SDcols = c('neural_stats_file','run_type','experiment.date','plate.id')], by = 'neural_stats_file', all = T)

# Check for uniqueness
set.tb[, num_unique_by_exp_plate := length(unique(setting_val)), 
       by = .(maestro_type, setting_type, setting, experiment.date, plate.id)]
set.tb[num_unique_by_exp_plate != 1,  .N, by = .(maestro_type, setting_type, setting)]
#    maestro_type         setting_type             setting  N
# 1:  Maestro Pro Maestro Pro Settings CO2 Conc. Set Point  4
# 2:  Maestro Pro Maestro Pro Settings    Actual CO2 Conc. 24
# 3:  Maestro Pro Maestro Pro Settings Current Temperature  6

# View variability in these cases
set.tb[num_unique_by_exp_plate != 1 & !setting %in% c('Original File Time','Experiment Start Time'),  .N, by = .(maestro_type, setting_type, setting, setting_val)][order(maestro_type, setting_type, setting, setting_val)]
```

None of these look truly concerning.


Check for setting_types with inconsistent setting values.

```{r eval = F}
set.tb[, num_unique_vals := length(unique(setting_val)),
       by = .(setting_type, setting)]
set.tb[num_unique_vals != 1, 
       .(maestro_types = paste0(unique(maestro_type),collapse = ",")),
       by = .(setting_type, setting, num_unique_vals)]
#                     setting_type                                   setting num_unique_vals       maestro_types
#  1:         Maestro Pro Settings                     Temperature Set Point               2         Maestro Pro
#  2:         Maestro Pro Settings                       Current Temperature               4         Maestro Pro
#  3:         Maestro Pro Settings                       CO2 Conc. Set Point               2         Maestro Pro
#  4:         Maestro Pro Settings                          Actual CO2 Conc.               6         Maestro Pro
#  5:         Maestro Pro Settings                              AxIS Version               3         Maestro Pro
#  6:         Maestro Pro Settings                      Maestro Pro Firmware               3         Maestro Pro
#  7:      Digital Filter Settings                     Low Pass Cutoff Freq.               2 Maestro Pro,Maestro
#  8:      Spike Detector Settings                                 Threshold               2 Maestro Pro,Maestro
#  9:      Burst Detector Settings Minimum Number of Spikes (network bursts)               2 Maestro Pro,Maestro
# 10: Statistics Compiler Settings                       Include Source Data               2 Maestro Pro,Maestro
```

I see that the maestro type is contributing to the variability in some settings. I feel comfortable assuming that whatever settings are common for the older maestro were used for these 6 files. So I will check that the settings are internally consistent for each maestro type. Later, I will do a bigger analysis to confirm that data from the Maestro and Maestro Pro are comparable.

Check for maestro_type - setting_type combinations with inconsistent setting values.

```{r eval = F}
set.tb[, num_unique_vals := length(unique(setting_val)),
       by = .(maestro_type, setting_type, setting)]
set.tb[num_unique_vals != 1, 
       .N,
       by = .(maestro_type, setting_type, setting, num_unique_vals)]
#            setting_type               setting num_unique_vals   N
# 1: Maestro Pro Settings Temperature Set Point               2 212
# 2: Maestro Pro Settings   Current Temperature               4 212
# 3: Maestro Pro Settings   CO2 Conc. Set Point               2 212
# 4: Maestro Pro Settings      Actual CO2 Conc.               6 212
# 5: Maestro Pro Settings          AxIS Version               3 212
# 6: Maestro Pro Settings  Maestro Pro Firmware               3 212
```

**Investigate each case:**

* Temperature Set Point

```{r eval = F}
# Temperature Set Point
set.tb[num_unique_vals != 1 & setting == 'Temperature Set Point', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
# Looks like the temperature was intentionally changed at some point..
set.tb[, culture.date := stri_extract(neural_stats_file, regex = '[0-9]{8}')]
set.tb[setting == 'Temperature Set Point', .(.N,num_files = length(unique(neural_stats_file))), by = .(culture.date, setting_val)][order(culture.date)]
```
Looks like they intentionally switched from 37 C to 35 C starting on 20210512
(then switched back to 37 C for the last culture). 
I am making a note to look into this more once I have all data, to see how much temp affects controls, and if data is combinable

* Current Temperature

```{r eval = F}
# Current Temperature
set.tb[num_unique_vals != 1 & setting == 'Current Temperature', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```
Variability within +/-0.1 C is not a concern. Larger differences most likely correspond to variable temperature set points, addressed above.

* CO2 Conc. Set Point

```{r eval = F}
set.tb[num_unique_vals != 1 & setting == 'CO2 Conc. Set Point', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```
This is fine

* Actual CO2 Conc.

```{r eval = F}
set.tb[num_unique_vals != 1 & setting == 'Actual CO2 Conc.', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```
Again, this variability is fine, and I will check on the plate that did not have Co2

* AxIS Version

```{r eval = F}
set.tb[num_unique_vals != 1 & setting == 'AxIS Version', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```
I know that we have used variables AxIS Versions to date, so we have to live with it.

* Maestro Pro Firmware

```{r eval = F}
set.tb[num_unique_vals != 1 & setting == 'Maestro Pro Firmware', .N, by = .(maestro_type, setting_type, setting, setting_val)][order(setting_type, setting, -N)]
```

Making a note to follow up.

Save set.tb for follow-up analyses

```{r eval = F}
setkey(set.tb, NULL)
set.tb.info <- paste0('Table containing the settings extracted from the Neural Statistics Compiler file headers. Made in run_me_TSCA2019.Rmd. Saved on ',Sys.Date())
save(set.tb, set.tb.info, file = file.path(project_name,'output','neural_stats_settings_table.RData'))
```


## Check analysis duration

Check that the analysis duration does not vary by more than 1% from the target (40 min, 2400 seconds)

```{r eval = F}
stopifnot(nrow(dat1[abs((analysis_duration - 2400)/2400) > 0.01]) == 0)
```

Any weirdness in the analysis start?

```{r eval = F}
dat1[, .N, by = .(analysis_start)]
```

All starting at 0, this is fine.

## Determine culture.date & group

Can get culture.date from the file name itself or from the file foldername. I'm finding that the foldername tends to be a bit more reliable (less prone to typos).

```{r eval = F}
# Get full file names to get culture date from 
files <- read_files(project_name)
files.tb <- data.table(fullname = files)
files.tb[, basename := basename(fullname)]
files.tb[, culture_folder := basename(dirname(dirname(fullname)))]
files.tb[, group_char := stri_extract(culture_folder, regex = 'G[0-9]+')]
files.tb[, group_int := stri_extract(group_char, regex = '[0-9]+')]
files.tb[, culture.date := stri_extract(culture_folder, regex = '[0-9]{8}')]
files.tb[, .N, by = .(culture.date)]
dat1 <- merge(dat1, files.tb, by.x = 'neural_stats_file', by.y = 'basename', all.x = T)

# Confirm days from culture to experiment date are within 13-15
# (if not, either there is a typo, or need to consider if data is similar enough to other expects performed in DIV 13-15)
dat1[, days_to_exp := as.numeric(as.Date(experiment.date, format = '%Y%m%d'))
     - as.numeric(as.Date(culture.date, format = '%Y%m%d'))]
dat1[, .(num_files= length(unique(neural_stats_file))), by = .(days_to_exp)][order(-num_files)]
#    days_to_exp  V1
# 1:          13 110
# 2:          15 102
# 3:          14   6
# looks good/reasonable!

# Compare the culture date from foldername to culture date in file name
# Resolve discrepancies
dat1[, culture.date.file := stri_extract(neural_stats_file, regex = '[0-9]{8}')]
dat1[culture.date != culture.date.file, .(length(unique(neural_stats_file))), by = .(culture_folder, culture.date.folder = culture.date, culture.date.file, experiment.date, days_to_exp)]
#                                   culture_folder culture.date.folder culture.date.file experiment.date days_to_exp V1
# 1:                          20210310 Culture G13            20210310          20210317        20210325          15  4
# 2: 20210324 Culture G14 (199E02 may need repeat)            20210324          20210331        20210406          13  6
# 3: 20210414 Culture G19 (200A08 may need repeat)            20210414          20210429        20210429          15  6
# 4: 20210602 Culture G28 (201E01, 201F04, 201F05)            20210602          20210526        20210615          13  6
# 5:                          20210602 Culture G29            20210602          20210526        20210616          14  6

```


G13 - lab notebook clearly indicates that culture.date was 2021-03-10 for all plates. I'm faily confident that the culture.date.file was just a typo
G14 - again, lab notebook agrees with culture date from folder name
G19 - again, lab notebook agrees with culture date from folder name. Looks like culture.date.file got changed to experiment.date
G28 - again, lab notebook agrees with culture date from folder name. 20210526 was the culture date of the previous culture, so likely the files names just go copied over.
G29 - again, lab notebook agrees with culture date from folder name. 20210526 was the culture date of the previous culture, so likely the files names just go copied over.

So in every cases, the culture date in the folder name agrees with the lab notebook. I'm going to assume that this is correct, and that the culture.date form the file names that are different are typos.

```{r eval = F}
# finalize the culture.date, get rid of extra columns
# culture.date (from folder) does not need any changes
dat1[, culture.date.file := NULL]
```

## Save updated dat1

```{r eval = F}
setkey(dat1, NULL)
save(dat1, file = paste0(project_name, "/output/",project_name,"_dat1.RData"))
rm(list = setdiff(ls(), keep.items))
```


# Level 2 

## Calculate % change in activity values

```{r eval = F}
# collapse the plate data by calculating the percent change in activity (dat2)
dat2 <- calcActivityPercentChange(project_name)

# check it out
dat2[wllq_lvl2==1, summary(rval)]
```

Minimum rval should be approximately -100 (slightly below -100 is normal), max is usually Inf.

## Save updated dat2

```{r eval = F}
setkey(dat2, NULL)
save(dat2, file = file.path(project_name,'output',paste0(project_name,"_dat2.RData")))
rm(list = setdiff(ls(), keep.items))
```

# Level 3

## Read in raw cytotox values

Get list of cytotox files

```{r eval = F}
raw.cytotox.files <- read_files(project_name, files_type = 'raw_cytotox')
```

The functions that read the raw data for the LDH and AB acute assays are based on the following assumptions:

**LDH**

The function `read_acute_LDH_raw_data` assumes that:

* All plates are 96-well (8x12)
* In each LDH raw data file, fo every plate, in column 1, there is a non-NA entry that contains the plate info (e.g., "20201104_20201117_MW71-7104_TSCA Acute DR_Plate 1")
* For every plate, in column 2, there is a cell that contains the word "Temperature"
* Raw data values for each plate start 1 row below every occurrence of the word "Temperature" and continue for the next 7 rows
* Raw data values for all plates are in columns 3 - 14

**AB**:

The function `read_acute_AlamarBlue_raw_data` assumes that:

* There is 1 plate per file
* Plates are 96-well (8x12)
* There is a cell with the letter "A" in the first column that signifies the first row of the plate
* Raw data values start in the same row as the letter "A" in column 1 and continue for the next 7 rows
* Raw data values are in columns B:M (i.e., 2 - 13)


There are some checks in the functions to see if each file passes these assumptions. If a fail fails these checks, the function will print a statement with a description of the fail. But, a file could have some unexpected data alignment that is not caught by the checks. Thus, these functions are not 100% reliable to read in the raw data, and the values should be compared to the Calculations files.

If a file fails the checks, either fix the data alignment in the file or just use the data values from the Calculations files (after manually confirming that the values in the Calculations files match the raw data values for that culture). 


Read raw data from LDH files
```{r eval = F}
raw.cytotox.files.LDH <- grep('LDH',raw.cytotox.files, val = T)
rawdat.LDH <- data.table()
for (filei in raw.cytotox.files.LDH) {
  add.tb <- read_acute_LDH_raw_data(filei)
  rawdat.LDH <- rbind(rawdat.LDH, add.tb)
}

# # To debug an individual file:
# filei
# debugonce(read_acute_LDH_raw_data)
# add.tb <- read_acute_LDH_raw_data(filei)
```
Note any LDH files that could not be read by the function read_acute_LDH_raw_data
```{r eval = F}
setdiff(basename(raw.cytotox.files.LDH), rawdat.LDH$filename)
```

Read raw data from AB files
```{r eval = F}
raw.cytotox.files.AB <- raw.cytotox.files[!grepl('LDH',raw.cytotox.files)]
rawdat.AB <- data.table()
for (filei in raw.cytotox.files.AB) {
  add.tb <- read_acute_AlamarBlue_raw_data(filei)
  rawdat.AB <- rbind(rawdat.AB, add.tb)
}

# # To debug an individual file:
# filei
# debugonce(read_acute_AlamarBlue_data)
# add.tb <- read_acute_AlamarBlue_data(filei)
```

Note any AB files that could not be read (manually check that these data values have been correctly copied over to the Calculations file)
```{r eval = F}
setdiff(basename(raw.cytotox.files.AB), rawdat.AB$filename)
```

Confirm expected # of raw data values per file

```{r eval = F}
rawdat.AB[, .N, by =.(filename)][N != 96]
rawdat.LDH[, .N, by =.(filename)][N != 96*3]
```


Get plate IDs for raw AB and LDH data, then combine

```{r eval = F}
# LDH

# Get plate.id from plate.info (taken from column A of each file) as:
# (1-2 digits)-(3-4 digits)
rawdat.LDH[, plate.id := stri_extract(plate.info, regex = '[0-9]{1,2}\\-[0-9]{3,4}')]
rawdat.LDH[, plate.id := paste0('MW',plate.id)]
rawdat.LDH[, .N, by = .(plate.id)] # confirm these looks correct

# AB

# Get plate.id from filename as:
# (1-2 digits)-(3-4 digits)
rawdat.AB[, plate.id := stri_extract(filename, regex = '[0-9]{1,2}\\-[0-9]{3,4}')]
rawdat.AB[, plate.id := paste0('MW',plate.id)]
rawdat.AB[, .N, by = .(plate.id)] # confirm these looks correct


# Combine cyto data
rawdat.cytotox <- rbind(rawdat.LDH, rawdat.AB, fill = T)
```

Get the culture date from the folders for raw cytotox data

```{r eval = F}
# Get full file names to get culture dates 
files <- read_files(project_name, files = 'raw_cytotox')
files.tb <- data.table(fullname = raw.cytotox.files)
files.tb[, basename := basename(fullname)]
files.tb[, culture_folder := basename(dirname(dirname(fullname)))]
files.tb[, group_char := stri_extract(culture_folder, regex = 'G[0-9]+')]
files.tb[, group_int := stri_extract(group_char, regex = '[0-9]+')]
files.tb[, culture.date := stri_extract(culture_folder, regex = '[0-9]{8}')]
files.tb[, .N, by = .(culture.date)]
rawdat.cytotox <- merge(rawdat.cytotox, files.tb, 
                        by.x = 'filename', by.y = 'basename', all.x = T)
```

Confirm 3 unique plates of each assay per group
```{r eval = F}
rawdat.cytotox[, .(num_unique_plate_ids = length(unique(plate.id))), by = .(acsn, culture_folder, group_char)][num_unique_plate_ids != 3]
# (if any cases, could be that there was a typo in the plate.id in the filename for AB or in the file header for LDH. Check it out and correct as needed)
```

Add the official acnm's

```{r eval = F}
rawdat.cytotox <- merge(rawdat.cytotox, acsn_map[, .(acsn, acnm)], by = 'acsn', all.x = T)
```


## Re-orient plates as needed

* For G11, reverse the plate orientation for AB Plate 71-7015 (not implemented in raw or calc file) *** (flip 180 degrees. Well A8 -> F1)
* For G20 plate 75-8201, reverse the plate orientation for AB (not implemented in raw or calc file) *** (flip 180 degrees. Well A8 -> F1)

```{r eval = F}
# confirm all rows and cols are defined
rawdat.cytotox[, .N, by = .(is.na(coli), is.na(rowi))]
rawdat.cytotox[, `:=`(rowi = as.integer(rowi), coli = as.integer(coli))]
rawdat.cytotox[, well_org := paste0(LETTERS[rowi],coli)]

# Save copy of current row & col
rawdat.cytotox[, `:=`(rowi_org  = rowi, coli_org = coli)]

# Switch the rows and columns for the upper left 48 wells
rawdat.cytotox[group_int == '11' & plate.id == 'MW71-7015' & acsn == 'AB' 
               & rowi %in% c(1:6) & coli %in% c(1:8), 
               `:=`(rowi = 7 - rowi_org,
                    coli = 9 - coli_org)]
rawdat.cytotox[group_int == '20' & plate.id == 'MW75-8201' & acsn == 'AB'
               & rowi %in% c(1:6) & coli %in% c(1:8), 
               `:=`(rowi = 7 - rowi_org,
                    coli = 9 - coli_org)]
```

Visually confirm G11 MW71-7015 rotation looks correct

```{r eval = F}
test.plate <- dcast(rawdat.cytotox[group_int == '11' & plate.id == 'MW71-7015' & acsn == 'AB'], rowi ~ coli, value.var = 'well_org')
test.plate
```


Since the calculations files have not already been corrected to reflect the rotation (since this is difficult to do in Excel), I will create .csv files with the raw values that have been rotated, then manually copy-paste in the Calculations files.


```{r eval = F eval = FALSE}
# Group 11, plate 7015
fullname.org <- rawdat.cytotox[group_int == '11' & plate.id == 'MW71-7015' & acsn == 'AB', unique(fullname)]
fullname.adj <- sub('\\.xls','_upper_48well_raw_vals_rotated_180degrees.csv',fullname.org)
tb1 <- dcast(rawdat.cytotox[group_int == '11' & plate.id == 'MW71-7015' & acsn == 'AB'], 
             rowi ~ coli, value.var = 'rval_not_blank_corrected')
tb1[, row_char := LETTERS[rowi]]
write.csv(tb1[, c('row_char', 1:12)], row.names = F, file = fullname.adj)

# Group 20, plate MW75-8201
fullname.org <- rawdat.cytotox[group_int == '20' & plate.id == 'MW75-8201' & acsn == 'AB', unique(fullname)]
fullname.adj <- sub('\\.xls','_upper_48well_raw_vals_rotated_180degrees.csv',fullname.org)
tb1 <- dcast(rawdat.cytotox[group_int == '20' & plate.id == 'MW75-8201' & acsn == 'AB'], 
             rowi ~ coli, value.var = 'rval_not_blank_corrected')
tb1[, row_char := LETTERS[rowi]]
write.csv(tb1[, c('row_char', 1:12)], row.names = F, file = fullname.adj)

```


## Calculate blank-corrected values

Calculate the average of the blank wells on each plate. Blank wells in both assays are in G1, 2 and 3

```{r eval = F}
# check that there are 3 non-NA blanks per plate
stopifnot(nrow(rawdat.cytotox[, .(sum(rowi == 7 & coli %in% c(1:3) & !is.na(rval_not_blank_corrected))), by = .(culture.date, plate.id, acsn)][V1 != 3]) == 0)

rawdat.cytotox[, blank_well_avg := mean(rval_not_blank_corrected[rowi == 7 & coli %in% c(1:3)], na.rm = T),
               by = .(culture.date, plate.id, acsn)]

```

Manually check the well quality tables to see if any blank wells have wllq == 0. If any, update the average of the blank wells on affected plates.

-> Culture 20210414 plate 75-8114 LDH well G3 has wllq == 0, so will exclude that well from blank well calculation

```{r eval = F}
# View values
rawdat.cytotox[culture.date == '20210414' & plate.id == 'MW75-8114' & acsn == 'LDH' & rowi %in% c(7) & coli %in% 1:3, .(rowi, coli, rval_not_blank_corrected)]

# Update blank well average to only include G1 and G2 on affected plate
rawdat.cytotox[culture.date == '20210414' & plate.id == 'MW75-8114' & acsn == 'LDH',  
               blank_well_avg := mean(rval_not_blank_corrected[rowi == 7 & coli %in% c(1,2)], na.rm = T),
               by = .(culture.date, plate.id, acsn)]
```

Subtract the average of the blank wells from all other wells

```{r eval = F}
rawdat.cytotox[, rval := rval_not_blank_corrected - blank_well_avg]
```


## Read in blank-corrected cytotox data from Calculations files

Note that the function `getAllCytoData` gets the blank-corrected values from the Calculations files. 

```{r eval = F}
# get cytotox data
cytodat <- getAllCytoData(project_name)
# some values are negative (-1081 - -3.3333e-05):
#                         acnm rval_is_neg    N
# 1:  CCTE_Shafer_MEA_acute_AB       FALSE 5277
# 2: CCTE_Shafer_MEA_acute_LDH        TRUE 2566
# 3: CCTE_Shafer_MEA_acute_LDH       FALSE 3649
# 4:  CCTE_Shafer_MEA_acute_AB        TRUE   51
# These will be set to 0
# 
# cytodat is ready
# Warning message:
# In valuesUnderTagPhrase(assay_dat_dt, tagPhrase = value_tagPhrase,  : 
#  Some LDH rval on 75-8114 are NA (may include some cells that are not needed for final cytodat)

```


Get culture dates

```{r eval = F}
# Get full file names to get culture date from 
files <- read_files(project_name, files = 'calculations')
files.tb <- data.table(fullname = files)
files.tb[, basename := basename(fullname)]
files.tb[, culture_folder := basename(dirname(fullname))]
files.tb[, group_char := stri_extract(culture_folder, regex = 'G[0-9]+')]
files.tb[, group_int := stri_extract(group_char, regex = '[0-9]+')]
files.tb[, culture.date := stri_extract(culture_folder, regex = '[0-9]{8}')]
files.tb[, .N, by = .(culture.date)]
cytodat <- merge(cytodat, files.tb, 
                 by.x = 'srcf', by.y = 'basename', all.x = T)

# Confirm 3 unique plates of each assay per group
cytodat[, .(num_unique_plate_ids = length(unique(plate.id))), by = .(acnm, culture_folder, group_char)][num_unique_plate_ids != 3]
# (if any cases, could be that there was a typo in the plate.id in the filename for AB or in the file header for LDH. Check it out and correct as needed)
```


## Compare raw and manipulated cytotox data, resolve discrepancies

```{r eval = F}
# Subset rawdat.cytotox to wells with data
rawdat.cytotox[, use_wells := 0]
rawdat.cytotox[acsn == 'AB' & rowi %in% c(1:6) & coli %in% c(1:8), use_wells:=1]
rawdat.cytotox[acsn == 'LDH' & ((rowi %in% c(1:6) & coli %in% c(1:8)) |
                                  (rowi == 7 & coli %in% c(4,5)) |
                                  (rowi == 8 & coli %in% c(1:6))), use_wells := 1]
rawdat.cytotox.tomerge <- rawdat.cytotox[use_wells == 1]

# Prepare to merge
cat(intersect(names(cytodat), names(rawdat.cytotox.tomerge)), sep = '", "')
cytodat[, in_calc := 1]
rawdat.cytotox.tomerge[, in_raw := 1]
nrow(cytodat)
nrow(rawdat.cytotox.tomerge)
comp.dat <- merge(cytodat, rawdat.cytotox.tomerge, by = c("plate.id", "rowi", "coli", "acnm",  "culture_folder", "group_char", "group_int", "culture.date"),
                  suffixies = c('.calc','.raw'), all = T)
```

### Compare data presence agreement

```{r eval = F}
comp.dat3[, .N, by = .(in_calc, in_raw)]
#    in_calc in_raw     N
# 1:       1      1 11439
# 2:      NA      1   105
# 3:       1     NA   104

setdiff(cytodat$culture.date, rawdat.cytotox$culture.date) # empty
setdiff(rawdat.cytotox$culture.date, cytodat$culture.date) # empty

setdiff(cytodat$plate.id, rawdat.cytotox$plate.id) # "MW75-9201"
setdiff(rawdat.cytotox$plate.id, cytodat$plate.id) # "MW75-9121"

comp.dat3[plate.id %in% c("MW75-9201",
                          "MW75-9121"), .N, by  = .(culture.date, plate.id, in_calc, in_raw, srcf, filename, acnm)][order(culture.date)]

```

For G36 - lab notebook and neural stats compiler files indicate that the plate.id for plate 3 is 75-9201, not 75-9121, so I think MW75-9201 is correct.

```{r eval = F}
rawdat.cytotox.tomerge[group_int == 36 & plate.id == 'MW75-9121', plate.id := 'MW75-9201']
```

Re-merge

```{r eval = F}
comp.dat <- merge(cytodat, rawdat.cytotox.tomerge, by = c("plate.id", "rowi", "coli", "acnm",  "culture_folder", "group_char", "group_int", "culture.date"),
                  suffixies = c('.calc','.raw'), all = T)

# Test data presence agreement
comp.dat3[, .N, by = .(in_calc, in_raw)]
#    in_calc in_raw     N
# 1:       1      1 11543
# 2:      NA      1     1

comp.dat3[is.na(in_calc), .(acnm, culture.date, plate.id, rowi, coli, rval.x, rval.y)]
#                         acnm culture.date  plate.id rowi coli rval.x    rval.y
# 1: CCTE_Shafer_MEA_acute_LDH     20210414 MW75-8114    8    6     NA 0.8766333

```
Ah, yes, this is the plate where there were only two 1/2 lysis control wells instead of the usual 3. Wllq will be 0 for this well regardless, so no updates are needed.

### Compare rvals

```{r eval = F}
comp.dat3[, rval_pct_diff := abs(rval.x - rval.y)/((rval.x + rval.y)*0.5)*100]

# See where the blank-corrected rvals differ by more tha 0.5% between the 2 source files
comp.dat3[rval_pct_diff >= 0.5, 
          .(.N, affected = paste0(unique(acsn),collapse = ","), 
            max_pct_diff = max(rval_pct_diff), 
            max_raw_diff = max(abs(rval.x - rval.y))), by = .(group_int, culture.date, plate.id)][order(culture.date, group_int)]
#    group_int culture.date  plate.id  N affected max_pct_diff max_raw_diff
# 1:         1     20201104 MW71-7106 17      LDH  2955.811277 8.562000e-01
# 2:         6     20210120 MW72-8208 18      LDH  1199.556541 9.918333e-01
# 3:         7     20210120 MW72-8212 20      LDH  3424.811219 1.058267e+00
# 4:        13     20210310 MW75-8003  1      LDH   200.000000 2.775558e-17
# 5:        26     20210526 MW75-8219 47       AB     4.042701 9.776667e+02

```

**Review all discrepancies**

* Group 1 - The blank wells were abnormally high for Group 1, plate 3 - therefore, the blanks from plate 2 were used instead in the Calculations file (2/3 blanks were ~10X those in plate 2). I agree with this decision
* Group 6 - similar situation, average of blanks taken from plate 2
* G7 - blanks for plate 2 taken from plate 1
* G13 - this is fine, actually difference is negligible (% diff just seems large because values are close to 0).
* G26, AB - blanks taken from plate 2

(Note that the above checks identified 3 additional groups with plates that did not have the correct raw data. These Calculations files were updated accordingly so that the Calculations and raw data files are now in agreement.)

## Identify final cytotoxiciy data table

In this case, we want the final values in the cytotoxicity data taken from the Calculations files.

```{r eval = F}
rm(rawdat.cytotox, rawdat.AB, rawdat.LDH, rawdat.cytotox.tomerge)

# keeping cytodat
cytodat[, in_calc := NULL]
setkey(cytodat, NULL)
save(cytodat, 
     file = paste0(project_name, "/output/",project_name,"_cytodat.RData"))
rm(list = setdiff(ls(), keep.items))

```

# Level 4

## Combine dat2 and cytodat

```{r}
# Load dat2
load(file.path(project_name,'output',paste0(project_name,"_dat2.RData")))

# Load cytodat
load(file.path(project_name,'output',paste0(project_name,"_cytodat.RData")))

# create a date_plate column to keep track of unique plates
dat2[, date_plate := paste(experiment.date,plate.id,sep = "_")]
cytodat[, date_plate := paste(experiment.date,plate.id,sep = "_")]

# Check if any plates in cytodat are not in dat2
cytodat[!date_plate %in% unique(dat2$date_plate), .N, by = .(culture_folder, experiment.date, plate.id)]
#                                   culture_folder experiment.date  plate.id   N
# 1:                           20201125 Culture G2        20201208 MW71-7113 104
# 2: 20210512 Culture G24 (200H02 may need repeat)        20210525 MW75-8213 104
```

I'm intentionally skipping 71-7113 and MW75-8213 in the MEA data for now, because of the split recordings.

```{r}
# Check if any plates in dat2 are not in cytodat
dat2[!date_plate %in% cytodat$date_plate, .N, by = .(culture_folder, experiment.date, plate.id)]
# empty
```

Combine into 1 table

```{r}
# clean columns
dat2[, srcf := paste(neural_stats_file.b,neural_stats_file.t,sep =";")]
dat2[, c("neural_stats_file.b","neural_stats_file.t") := NULL]
cytodat[, fullname := NULL]
setdiff(names(dat2), names(cytodat)) # can fill wllq_lvl2 with NA for now
setdiff(names(cytodat), names(dat2)) # can fill treatment, conc with NA for now 

dat3 <- rbind(dat2, cytodat, fill = T)
rm(dat2, cytodat)
```


Initialize columns that will be useful for checks below

```{r}
# Define the full well id to faciliate checking
dat3[, full_well_id := paste(apid, rowi, coli, sep = '_')]
```

## Check treatment labels

```{r}
# save the original treatment name as read from the source file (srcf) for reference
dat3[, treatment_srcf := treatment]
```

* Need to get the treatment labels for the dat2 from the cytodat (because no MaestroExperimentLog saved). But first, confirm that the treatments in the LDH and AB data agree by well.

```{r}
dat3[grepl('(LDH)|(AB)',acnm), 
     .(num_treatment_labels = length(unique(treatment))), 
     by = .(experiment.date, plate.id, rowi, coli)][num_treatment_labels > 1]
# (should be empty)
# if there was a real experimental inconsistency in the treatments by well in the LDH and AB, do something creative to extrapolate the correct treatment labels for the other parameters
```

* Check for any missing treatment labels. Usually, any NAs would correspond to blank or Media-only wells, but confirm there are no unexpected NA treatments.

```{r}
dat3[grepl('(LDH)|(AB)',acnm) & (is.na(treatment) | treatment %in% 0)]
# dat3[is.na(treatment), treatment := 'Media']
```

What is this 0 treatment?

```{r}
dat3[treatment == 0, .N, by = .(culture_folder, rowi)][order(culture_folder)]
```

Lab notebook confirms that the wells E5 - E8 and F5 - F8 appear to have been empty in G32, G33, G34, G35. Will re-label these wells as "Media"

```{r}
dat3[culture_folder %in% c('20210630 Culture G32','20210707 Culture G33','20210707 Culture G34','20210714 Culture G35') & coli %in% 5:8 & rowi %in% 5:6, treatment := 'Media']

dat3[treatment == 0, .N, by = .(treatment, culture_folder, rowi, srcf)][order(culture_folder)]

```

Looking at the Calculations file (sheet 'Dosing Plate'), I'm pretty sure this treatment is supposed to be 199B10. But I'm always hesitant to just update the meta data files on my own...



* Extrapolate the treatment labels for the non-cytotoxicity parameters

```{r}
dat3[!grepl('(LDH)|(AB)',acnm), .N, by = .(treatment)] # all currently NA

# extrapolate treatment labels
dat3[, treatment := unique(treatment[!is.na(treatment)]), by = .(experiment.date, plate.id, rowi, coli)]
dat3[is.na(treatment)]  #empty, good
```

* Standardize controls

```{r}
# If some DMSO wells labelled "DMSOa", "DMSOb", etc, standardize
dat3[grepl("DMSO",treatment), treatment := "DMSO"]
```

* Check the number of treatments per plate. Usually, there should be 10 unique treatments per MEA or AB plate (6 chemicals + DMSO + PICRO + TTX + Media) and 14 per LDH plate (10 + Lysis + 1/2 Lysis + 1:250 LDH + 1:2500 LDH). If there are more or less, check for any typos.

```{r}
dat3[!grepl('LDH',acnm), .(num_treatments_per_plate = length(unique(treatment))), by = .(apid)][num_treatments_per_plate != 10]

dat3[grepl('LDH',acnm), .(num_treatments_per_plate = length(unique(treatment))), by = .(apid)][num_treatments_per_plate != 14]
```

* Check for the expected number of wells associated with each treatment. Usually, for every treatment, there should be 3 plates * 7 wells per plate = 21 wells. If there are more or less than 21 wells, check for any typos in the treatment. Note that vehicle controls will be associated with 6 wells * number of plates. Some treatments may have been intentionally repeated in multiple cultures (in which case these will be associated with a multiple of 21 wells). Check that the treatments that are associated with multiple cultures appear are not unexpected). 

```{r}
dat3[, .(num_wells_per_treatment = length(unique(full_well_id))), 
     by = .(treatment)][num_wells_per_treatment != 21]

# Check that treatments that appear to have been repeated were tested in a group that indicates an intentional repeat (rather than a typo)
ignore.repeated.treatments <- c('DMSO','Water','EtOH','PICRO','TTX', 'Media','1:250 LDH','1:2500 LDH')
check.treatments <- dat3[, .(num_wells_per_treatment = length(unique(full_well_id))), by = .(treatment)][num_wells_per_treatment != 21 & !(treatment %in% ignore.repeated.treatments | grepl('Lysis',treatment)), unique(treatment)]
# check.treatments
dat3[treatment %in% check.treatments,
     .(cultures = paste0(sort(unique(culture.date)),collapse = ",")),
     by = .(treatment)]

dat3[treatment %in% check.treatments,
     .N, by = .(treatment, culture_folder)][order(treatment)]
```

For TSCA2019, I know that a lot of compounds had to be repeated. Will vet whether these treatments were intentional (versus typos) later on when we determine which experiments to keep for each treatment.


## Determine appropriate treatment labels for positive controls

### MEA plates

Picrotoxin (PICRO) and Tetrodotoxin (TTX) are added as positive controls that increase (PICRO) or decrease (TTX) the activity in the MEA plates. These positive controls should have little to no effect on the cell viability. 

PICRO is usually added to well D1, TTX to well E1, and Lysis to well F1. These positive controls may be added before or after the second recording. (Usually PICRO and TTX added before, Lysis added after). If they were added before the second recording, then we want the treatment labels in these wells to be PICRO and TTX, respectively. However, if they were added after the second recording, then the treatment labels for these wells should be "Media" (because no treatment was present in these wells during either second recording). The choice to add the PICRO and TTX before or after the second experiment may vary by experiment date.

```{r}
# visually confirm if the PICRO, TTX, LYSIS were added before the second recording for MEA endpoints
# varies across experiments, sometimes across days
plotdat <- dat3[(treatment %in% c("DMSO","PICRO","TTX","BIC","Media") | grepl("lysis",tolower(treatment))) & acnm == "CCTE_Shafer_MEA_acute_firing_rate_mean"]

ggplot(plotdat, aes(x = treatment, y = rval))+
  geom_jitter(width = 0.2, height = 0, pch = 1)+
  ylab('mean firing rate % change')+
  geom_hline(yintercept = -100, lty = 'dashed')+
  theme_bw()+
  ggtitle(paste0('% change in Mean Firing Rate for Controls',
                 '\nNO changes to treatment labels'))
```

If some of the PICRO, TTX, or LYSIS wells look clearly more like the DMSO or Media wells than positive controls, it is possible that those wells did not contain the positive control substance during the second recording and should be re-labelled as 'Media'. Use your best judgment. Regardless, the PICRO, TTX, and LYSIS wells are not currently used as part of the normalization for the acute MEA data, so it is not the end of the world if these treatment labels are incorrect.

Note that if some individual wells appear way off, check the well quality table to see if these will be excluded regardless.

For the TSCA2019:

* PICRO - seems like some wells responded really well, others not so much. But there isn't a clearly separation of data points where I would think that some wells did not have PICRO, so I can't defend an argument to update the treatment labels for the MEA data
* TTX - all but 2 wells had a complete loss of activity, so these treatment labels look correct.
* No data rows associated with the MEA data currently labelled as 'Lysis'

### AB plates

For cytotoxicity assays, the F1 wells should contain Lysis (usually says "Media" in meta data, since that is correct for the MEA plates). Re-label the treatments to refect this.

However, note that in TCPL the 'pval' for the Alamar Blue assay is set to 0. The wells with wllt == 'p' are not used as part of the normalization. Therefore, the Lysis wells will be ignored in the Alamar Blue assay. So getting this treatment label correct is not critical. 

```{r}
# for Cell Titer Blue assay:
plotdat <- dat3[(treatment %in% c("DMSO","PICRO","TTX","BIC","Media","1:250 LDH","1:2500 LDH") 
                 | grepl("lysis",tolower(treatment))
                 | (rowi = 6 & coli == 1)) & grepl("(AB)",acnm)]

# Separate out the F1 wells
plotdat[rowi == 6 & coli == 1, treatment := paste0(treatment,'_F1')]

ggplot(plotdat, aes(x = treatment, y = rval))+
  geom_jitter(width = 0.2, height = 0, pch = 1)+
  ylab('blank-corrected value')+
  theme_bw()+
  ggtitle(paste0('Alamar Blue blank-corrected values for Controls',
                 '\nNO changes to treatment labels'))
```

For the TSCA2019:

* Yep, looks all of the F1 wells have a blank-corrected value near 0. Therefore, the treatment in these wells was most likely Lysis. 
* PICRO and TTX wells look fairly similar to DMSO, as expected

```{r}
# Update treatment label
dat3[grepl("AB",acnm) & rowi == 6 & coli == 1, treatment := 'Lysis']

# View updated plot
plotdat <- dat3[(treatment %in% c("DMSO","PICRO","TTX","BIC","Media","1:250 LDH","1:2500 LDH") 
                 | grepl("lysis",tolower(treatment))
                 | (rowi = 6 & coli == 1)) & grepl("(AB)",acnm)]

ggplot(plotdat, aes(x = treatment, y = rval))+
  geom_jitter(width = 0.2, height = 0, pch = 1)+
  ylab('blank-corrected value')+
  theme_bw()+
  ggtitle(paste0('Alamar Blue blank-corrected values for Controls',
                 '\nF1 wells re-labelled as "Lysis"'))

```

### LDH plates

Treatments in the LDH wells are generally the same as in the MEA plates and should not need to be updated (see experiment details). Note that in TCPL the 'pval' for the LDH assay is defined by the median the Lysis wells on each apid.

```{r}
# for LDH assay:
plotdat <- dat3[(treatment %in% c("DMSO","PICRO","TTX","BIC","Media","1:250 LDH","1:2500 LDH") 
                 | grepl("lysis",tolower(treatment))
                 | (rowi = 6 & coli == 1))
                & grepl("(LDH)",acnm)]

# Separate out the F1 wells
plotdat[rowi == 6 & coli == 1, treatment := paste0(treatment,'_F1')]

ggplot(plotdat, aes(x = treatment, y = rval))+
  geom_jitter(width = 0.2, height = 0, pch = 1)+
  ylab('blank-corrected value')+
  theme_bw()+
  ggtitle(paste0('LDH blank-corrected values for Controls',
                 '\nNO changes to treatment labels'))
```

Median, PICRO, TTX, and F1 wells generally appear similar to DMSO. There are a few outliers, but it's not worth investigating because the Media, PICRO, and TTX wells are not used to normalize regardless. The outlier DMSO wells are a concern... but if there is no specific well quality note, I have no way of knowing if the wllq or treatment labels should be updated.

**Maybe the DMSO should be investigated...?


```{r}
dat3[treatment %in% c('DMSO','Media','PICRO','TTX') & rval > 1 & grepl('LDH',acnm), .N, by = .(culture_folder, plate.id, treatment, rval)]
```

# RESUMe HERE

# ASSIGN SPIDS
```{r}
cat("\nAssign spid's:\n")
cat("Using spidmap file:",spidmap_file,"\n")
spidmap <- as.data.table(read_excel(spidmap_file, sheet = spid_sheet))
names(spidmap)
setnames(spidmap, old = "NCCT ID", new = "spid")
setnames(spidmap, old = "Chemical ID", new = "treatment")
setdiff(unique(dat3$treatment), unique(spidmap$treatment))
# [1]   
dat3 <- merge(x = dat3, y = spidmap[, c("spid", "treatment")], all.x = TRUE, by = "treatment")

# assign spids for the non-registered control compounds, e.g.: "Tritonx100" "Bicuculline"  "DMSO" "PICRO" "TTX" "MEDIA"
dat3[is.na(spid),unique(treatment)]
# [1] 
dat3[grepl("DMSO",treatment), spid := "DMSO"]
dat3[treatment == "Media", spid := "Media"]
dat3[treatment == "PICRO", spid := "Picrotoxin"]
dat3[treatment == "TTX", spid := "Tetrodotoxin"]
dat3[grepl("Lysis",treatment), spid := "Tritonx100"]
dat3[grepl("Lysis",treatment), unique(conc), by = "treatment"]
unique(dat3$spid) # confirm no NA spids
if(any(is.na(unique(dat3$spid)))) {
  stop(paste0("The following treatments don't have a corresponding spid:", dat3[is.na(spid), unique(treatment)]))
} else {
  cat("No spids are NA.\n")
}
cat("Number of unique spids:",dat3[,length(unique(spid))],"\n")
```

# PREPARE LDH P WELLS (must verify wllq, treatments first)
```{r}
dat3 <- prepare_LDH_p_wells(dat3)
```


# ASSIGN WLLT
```{r}
dat3 <- assign_wllt(dat3)
```


# CHECK CONC'S
```{r}

cat("\nFinalize Concentrations:\n")
dat3[, conc_original := conc]
dat3[, unique(conc)] # any NA's? any non-numeric? Any 0? does it look like conc correction was done?

# update conc for DMSO, PICRO, TTX, BIC, and full Lysis wells
# dmso
dat3[treatment == "DMSO",unique(conc)]
# [1] "Control"
# Use the percent DMSO by volume?
# dat3[treatment == "DMSO", conc := "0.001"]

# picro
dat3[treatment == "PICRO", .N, by = "conc"]
# 
# based on lab notebook, this is usually 25
# dat3[treatment == "PICRO", conc := "25"]

# ttx
dat3[treatment == "TTX", .N, by = "conc"]
# 
# based on lab notebook, this is usually 1
# dat3[treatment == "TTX", conc := "1"]

cat("\nConcentration Corrections:\n")
# any other compounds to update??
# need to do concentration correction??
cat("CHANGES MADE/rationale")
# cat("The following treatment have char conc. Will be set to NA:\n")
# print(suppressWarnings(dat3[is.na(as.numeric(conc)), .N, by = c("spid","treatment","conc")]))
# dat3[, conc := suppressWarnings(as.numeric(conc))]

# final updates, view conc's, make table of control conc's
dat3 <- assign_common_conc(dat3)
```

## FINALIZE WLLQ
```{r}
# From combine, above


cat("\nFinalize Wllq:")
# set wllq to zero where rval is NA
cat("\nNA rval's:",dat3[wllq==1 & is.na(rval),.N])
#
cat("\nInf rval's (baseline==0):",dat3[wllq==1 & is.infinite(rval),.N])
# 
dat3[is.na(rval), `:=` (wllq = 0, wllq_notes = paste0(wllq_notes, "rval is NA; "))]
dat3[is.infinite(rval), `:=` (wllq = 0, wllq_notes = paste0(wllq_notes, "rval is Inf; "))]
cat("\nWell quality set to 0 for these rval's.\n")

# do any other updates to wllq based on notes from lab notebook
# e.g. misdosed, recording too long, etc.
# for example, updateWllq(dat3, date = "20190530", plate = "MW68-0807", well = "C6", wllq_note = "Contamination", override_check = override_wllq_checks)
```

```{r}
# create a nice summary of wllq assignments for each well
createWllqSummary(dat3, project_name)
cat("(note that the wllq is not quite final -\nwllq will be updated for outlier DMSO wells will before creating lvl 0 snapshot)\n")
```



## Check all acnm are registered in invitrodb

**Replace below with just checking that all export_ready == 1 endpoitns are present


Note that different maestros (and possibly different axis versions) will output slightly different subsets of endpoints. So, just want to check that there are no unexpected new endpoints.

```{r}
setdiff(dat1$acsn, acsn_map[Status_2023.05.12 == 'registered in invitrodb',unique(acsn)])
```

"Network ISI Coefficient of Variation" - this is not an endpoint that has export_ready == 1, so this is okay (even though it is registered).

```{r}
# Note total parameters
dat1[, param_total := length(unique(acsn)), by = .(neural_stats_file)]
dat1[, registered_param := length(unique(acsn[acsn %in% acsn_map[Status_2023.05.12 == 'registered in invitrodb',acsn]])), by = .(neural_stats_file)]
dat1[, not_registered_param := length(unique(acsn[acsn %in% acsn_map[Status_2023.05.12 != 'registered in invitrodb',acsn]])), by = .(neural_stats_file)]
dat1[, .(num_files = length(unique(neural_stats_file))),
     by = .(param_total, registered_param, not_registered_param)]

# Note which of the registered parameters are missing (will vary by maestro type)
registered.params <- acsn_map[Status_2023.05.12 == 'registered in invitrodb',unique(acsn)]
res <- dat1[, .(missing_params = paste0(setdiff(registered.params, acsn),collapse = ",")), by = .(neural_stats_file, setting_axis.version)]
res[, .N, by = .(missing_params, setting_axis.version)]

```

I am confident that no one is haphazardly selected which parameters to include. I think it all depends on the versions of the machine/software. So we'll just include all provided info and keep rolling.

Confirm that the main 15 are present in every file?
# Note the vehicle controls per plate

Check if any plates have multiple vehicle controls

```{r}
# Check if any plates have multiple vehicle controls
# (if multiple are present on a given plate, consider the normalization method and/or apid definition for the given plate)
dat3[, num_vehicle_control_types_per_plate := length(unique(treatment[wllt == 'n'])), by = .(apid)]
dat3[num_vehicle_control_types_per_plate > 1]

# View the vehicle controls on these plates
# dat3[num_vehicle_control_types_per_plate > 1 & wllt == 'n', .N, by = .(apid, treatment, spid)]
```


# Check for NA rvals

Check for any NA or infinite rvals where wllq == 1. In the ToxCast pipeline, any NA rval will be set to wllq = 0 (at level 2).

Check for NA's in all parameters other than the 'DIV' parameters.
In these cases, if there are any NA rvals with wllq == 1, that indicates there was a problem in the data files, data collection, and/or pre-processing. These instances warrant further investigations
```{r}
dat3[(is.na(rval) | is.infinite(rval)) & wllq == 1 & !grepl('DIV',acnm), .N, by = .(acnm)]
stopifnot(nrow(dat3[is.na(rval) & wllq == 1 & !grepl('DIV',acnm)]) == 0)
```

Note NA's in the DIV-specific parameters. These can be left as NA, but just note the occurrence
```{r}
dat3[(is.na(rval) | is.infinite(rval)) & wllq == 1 & grepl('DIV',acnm), .N, by = .(acnm)]
```

# For repeated treatments, determine which culture(s) to keep

Sometimes a treatment is repeated in multiple cultures within a project. This is usually due to some unwanted results in the first culture, e.g., high variability, precipitate, or cytotoxicity. In any case, discuss with lab which culture(s) we want to keep for each repeated treatment. Make graphs to visualize as needed. For cultures that we don't want to keep for a given treatment, set wllq := 0 and add a wllq_note.

```{r}
# define the "assay" for every row of dat
dat3[grepl("LDH",acnm), assay := 'LDH']
dat3[grepl("AB",acnm), assay := 'AB']
dat3[is.na(assay), assay := 'NFA']

# Identify treatments tested in multiple cultures for a given assay
dat3[wllq == 1 & wllt == 't', .(num_cultures = length(unique(culture.date)),
                                cultures = paste0(sort(unique(culture.date)),collapse = ",")),
     by = .(treatment, assay)][num_cultures > 1]
```

# Confirm every treatment has some data with wllq=1

```{r}
all.treatments <- dat3[wllt == 't', unique(treatment)]
dat3[wllq == '1', setdiff(all.treatments, treatment)]
```

For this example - since we aren't including the entire data set, this is okay, this affected compound needed to be repeated

```{r}
dat3[treatment == '7126 B1', .N, by = .(culture.date, wllq, wllq_notes)]
```

# Check for the expected number of technical replicates for every treatment-conc 

Usually there are 3 technical replicates, unless the treatment was intentionally repeated. Again, this check serves to check for potential typos

```{r}
dat3[wllt == 't', 
     .(num_replicates = length(unique(full_well_id))), by = .(treatment, spid, conc)][num_replicates != 3]
```


# Data usability checks

Set wllq to 0 for apid where the activity in control wells is very low (specifically where the median of controls on DIV 12 is < 10 spikes per min or < 2 active electrodes).
```{r}
# Plates with MFR < 10 spikes per minute on DIV 12
apid.low.mfr <- dat3[wllq == 1 & wllt == 'n' & grepl("firing_rate_mean_DIV12",acnm),
                     .(bval = median(rval, na.rm = T)), 
                     by = .(apid)][bval < 10/60, unique(apid)]

# Plates with < 2 active electrodes on DIV 12
apid.low.ae <- dat3[wllq == 1 & wllt == 'n' & grepl("active_electrodes_number_DIV12",acnm), 
                    .(bval = median(rval, na.rm = T)), 
                    by = .(apid)][bval < 2, unique(apid)]

# View affected plates
exclude.apid <- union(apid.low.mfr, apid.low.ae)
cat('Plates with very low activity on DIV 12 for control wells: ')
print(dat3[apid %in% exclude.apid & (grepl("active_electrodes_number_DIV12",acnm) | grepl("firing_rate_mean_DIV12",acnm)) & wllt == "n", 
           .(apid, acnm_short, rval)])

# Set wllq to 0 for affected plates
dat3[apid %in% exclude.apid, `:=`(wllq = 0,
                                  wllq_notes = paste0("Controls low activity on DIV 12 (median MFR < 10 spikes per min or nAE < 2); ",wllq_notes))]


```


# Data summaries 

## Counts 

**Number of cultures dates:**

```{r}
dat3[, length(unique(sub("_.+$","",apid)))]
```

**Range of culture dates:***
```{r}
dat3[, range(sub("_.+$","",apid))]
```

**Number of plates tested:**
```{r}
dat3[, length(unique(apid))]
```

**Number of compounds tested:**
```{r}
dat3[wllt == "t", length(unique(spid))]
```

**Wllq counts for all data points:**
```{r}
print(dat3[, .N, by = "wllq"]) # note if wllq is NA anywhere
```

**Number of unique assay componentss present:**
(there should be 2 cyto + 17 AUC + 4*17 DIV = 87)
```{r}
length(unique(dat$acnm))
```

**Any plates don't have 48 wells for each component?** 
```{r}
dat3[, .N, by = .(apid, acnm)][N != 48]
```

**Any plates don't have 6 control wells for each component?**
```{r}
print(dat3[wllt == "n", .N, by = c("acnm","apid")][N != 6])
```

**Range of assay component values**
```{r}
dat3[wllq == 1, .(min = format(min(rval,na.rm=T),digits=2,scientific = F), 
                  median = format(median(rval,na.rm=T),digits=2,scientific = F),
                  max = format(max(rval,na.rm=T),digits=2,scientific = F),
                  num_NA = sum(is.na(rval))), by = .(acnm_short)][order(acnm_short)]
```

## Visualizations

```{r}

```


## Run dataset checks
```{r}

# check that all data is there, nothing is missing, view plots
data_checks(dat3)
```



# Save final table -nfa

acute
```{r}
# save dat3
dat3 <- dat3[, .(treatment, spid, experiment.date, plate.id, apid, rowi, coli, conc, acnm, wllt, wllq, wllq_notes, rval, srcf, dat3)]
save(dat3, file = file.path(main.output.dir, paste0("output/",project_name,"_dat3_",as.character.Date(Sys.Date()),".RData")))
cat("\ndat3 saved on:",as.character.Date(Sys.Date()), "\n")
```


```{r}
# Note that the cndx here is different than the cndx applied in tcpl
setnames(dat, old = 'cndx', new = 'cndx_pre_pro')

# Select columns to save
keep.columns <- c(
  
  # Columns needed for tcpl lvl0
  'acnm','apid', 'rowi', 'coli', 'conc','rval','srcf','spid','wllt','wllq', 
  
  # Additional columns for our documentation
  'acsn',  
  'treatment', 'treatment_srcf','spidmap_file',
  'cndx_pre_pro','conc_srcf', 'units',  'stock_conc', 'stock_conc_units',
  'wllq_notes', 'wllq_ref'
)
dat <- dat3[, .SD, .SDcols = keep.columns]

# Save the file
setkey(dat, NULL) # remove keys to make the file smaller
nfa.dat.description <- paste0(project_name,' MEA NFA pre-processed data
Date prepared: ',as.character.Date(Sys.Date()),
'\nTo do before tcplWriteLvl0:
* Exclude acnms that are for a specific DIV (see where grepl("_DIV",acnm))
* Check that none of the data has been pipelined before (i.e., merge with current level 0 data and check for duplicates by apid, rowi, coli, and acnm)')
cat(nfa.dat.description)
save(dat, nfa.dat.description, 
     file = file.path(root.output.dir, project_name, "output", paste0(project_name,"_for_tcpl_lvl0.RData")))
```

You made it!
